{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('names', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Load the CMU Pronouncing Dictionary from the internet\n",
    "url = \"http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b\"\n",
    "with urllib.request.urlopen(url) as f:\n",
    "    cmudict = f.read().decode(\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphonetics\n",
    "import pronouncing\n",
    "\n",
    "\n",
    "# Define a function that generates homophones for a given word\n",
    "def generate_homophones(word):\n",
    "    homophones = []\n",
    "    # Create an instance of the Soundex algorithm\n",
    "    soundex = pyphonetics.RefinedSoundex() #There are many others but this is the best one\n",
    "    # Convert the input word to its Soundex code\n",
    "    try:\n",
    "        soundex_code = soundex.phonetics(word)\n",
    "\n",
    "    # Compare the Soundex code of the input word with the Soundex code of each word in the CMU Pronouncing Dictionary\n",
    "        for line in cmudict.splitlines():\n",
    "            if line.startswith(\";;;\"):\n",
    "                continue\n",
    "            line = line.strip().split()\n",
    "            pron = \"\".join(line[1:])\n",
    "            pron_code = soundex.phonetics(pron)\n",
    "            if soundex_code == pron_code:\n",
    "                homophones.append(line[0])\n",
    "\n",
    "        # Return the list of homophones\n",
    "        homophones.extend(pronouncing.rhymes(word))\n",
    "    except IndexError:\n",
    "        return pronouncing.rhymes(word)\n",
    "    return list(set(homophones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homophones for adore: corr, livermore, non-core, paramore, roar, anti-war, goar, sore, war, boar, longcor, bore, spore, tor, fore, ngor, anymore, knorr, underscore, ashore, soar, snore, gilgore, mor, clore, loar, nor, roehr, implore, schor, cor, sedor, laur, nohr, lenore, espectador, d'or, boer, spaur, stohr, pour, torr, por, borre, doerr, balthazor, morr, ACERO(2), bensenyore, villasenor, hoerr, abor, ATARI, kohr, yore, elnore, offshore, gorr, lor, ADOREE, moore, door, labore, delore, gore, horr, drawer, forr, jambor, lahore, lalor, igor, chore, hoar, hoare, ADORA, faure, whore, schnorr, bohr, decor, guarantor, thor, dohr, livor, orr, lamaur, saur, score, glore, restore, oar, shor, wor, schorr, cavalli-sfor, stoehr, noncore, baur, rumore, heretofore, laure, wore, inshore, loehr, vore, lazor, devor, floor, more, pore, ohr, shorr, four, outpour, wherefore, galore, dior, coar, or, flor, rapport, deplore, swore, glor, sor, senor, cohr, mazor, store, sotomayor, shore, ador, ignore, montefiore, corps, underinsure, porr, timor, lohr, ore, outscore, abhor, montemayor, torre, flore, doar, rohr, explore, storr, core, woehr, scor, armentor, gabor, warr, AUDRIE, bator, tore, mohr, lore, before, for, roquemore, postwar, melor, dore, prewar, your, hardcore, antiwar, dorr\n"
     ]
    }
   ],
   "source": [
    "# Testing our thingy, works pretty good but has bizzare words \n",
    "word = \"adore\"\n",
    "\n",
    "# Generate homophones for the input word using the CMU Pronouncing Dictionary and Pyphonetics, loaded from the nettttt\n",
    "homophones = generate_homophones(word)\n",
    "\n",
    "# Liste of homophones\n",
    "if len(homophones) > 0:\n",
    "    print(f\"Homophones for {word}: {', '.join(homophones)}\")\n",
    "else:\n",
    "    print(f\"No homophones found for {word}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_english_word_or_name(word): #APIS sometimes return with names and soetimes return just false words\n",
    "\n",
    "    # Download the English names corpus if necessary\n",
    "    nltk.download('names', quiet=True)\n",
    "    \n",
    "    # Get a set of English words and names\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "    english_names = set(nltk.corpus.names.words())\n",
    "    \n",
    "    # Check if the word is in the set of English words or names\n",
    "    if word.lower() in english_words or word.title() in english_names:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_words(words): # Some word have special characters\n",
    "\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        clean_word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "        if clean_word and is_english_word_or_name(word):\n",
    "            clean_words.append(clean_word)\n",
    "    return [w.upper() for w in clean_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'homophones' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clean_words(homophones)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'homophones' is not defined"
     ]
    }
   ],
   "source": [
    "clean_words(homophones) # just testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Rania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.spatial.distance import cosine, euclidean, cityblock, jaccard\n",
    "\n",
    "# Load un corpus pour les mots\n",
    "nltk.download('brown')\n",
    "corpus = brown.words()\n",
    "\n",
    "# creer une distribution\n",
    "fdist = nltk.FreqDist(corpus)\n",
    "def dist_sim(word1, word2, metric='euclidean'): # Similarity to choose the most textually proche word\n",
    "    # get la freq du corpus\n",
    "    freq1 = fdist[word1.lower()]\n",
    "    freq2 = fdist[word2.lower()]\n",
    "\n",
    "    # get tous les mots du corpus\n",
    "    vocab = set(corpus)\n",
    "\n",
    "    # rep vect des mots\n",
    "    vec1 = [freq1 if w == word1 else 0 for w in vocab]\n",
    "    vec2 = [freq2 if w == word2 else 0 for w in vocab]\n",
    "\n",
    "    # Calculate similarity\n",
    "    if metric == 'cosine':\n",
    "        sim = 1 - cosine(vec1, vec2)\n",
    "    elif metric == 'euclidean':\n",
    "        sim = 1 / (1 + euclidean(vec1, vec2))\n",
    "    elif metric == 'manhattan':\n",
    "        sim = 1 / (1 + cityblock(vec1, vec2))\n",
    "    elif metric == 'jaccard':\n",
    "        sim = 1 / (1 + jaccard(vec1, vec2))\n",
    "    else:\n",
    "        raise ValueError('Unsupported distance metric')\n",
    "\n",
    "    return sim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def replace_word(input_word, word_list): # Choosing most similar word to replace with from list\n",
    "  \n",
    "    similarity_scores=[]\n",
    "    \n",
    "    for w in word_list: # we calculate textual similarity between each word in the list and our word \n",
    "        similarity_scores.append(dist_sim(input_word,w)) \n",
    "    \n",
    "    if len(similarity_scores)==0:\n",
    "        return None\n",
    "    # Get the index of the word in word_list with the highest cosine similarity to input_word\n",
    "    most_similar_word_index = np.argmax(similarity_scores)\n",
    "    \n",
    "    # Get the most similar word from word_list\n",
    "    most_similar_word = word_list[most_similar_word_index]\n",
    "    \n",
    "    return most_similar_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def replace_word(word, word_list):\n",
    "    if not word_list:\n",
    "        return word\n",
    "    else:\n",
    "        return random.choice(sorted(word_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def put_word(sentence, word, potential_replacement):\n",
    "    if potential_replacement is None or word is None:\n",
    "        return sentence\n",
    "    else:\n",
    "        return sentence.replace(\" \" + word + \" \", \" \" + potential_replacement + \" \")\n",
    "\n",
    "def generate_dataset(df, column_name):\n",
    "    \n",
    "    df[\"sentence 1\"] = df[column_name]\n",
    "    df[\"sentence 2\"] = df[column_name]\n",
    "    for i in range(len(df)):\n",
    "        if i<=2369: continue\n",
    "        sentence_1 = df.loc[i, \"sentence 1\"]\n",
    "        sentence_2 = df.loc[i, \"sentence 2\"]\n",
    "        words_1 = sentence_1.split()\n",
    "        words_2 = sentence_2.split()\n",
    "        n = random.randint(0, len(words_1))\n",
    "        m = random.randint(0, len(words_2) - n)\n",
    "        indices_1 = random.sample(range(len(words_1)), n)\n",
    "        indices_2 = random.sample([i for i in range(len(words_2)) if i not in indices_1], m)\n",
    "        for j in indices_1:\n",
    "            potential_replacement = replace_word(words_1[j],list(set(clean_words(generate_homophones(words_1[j])))))\n",
    "            sentence_1 = put_word(sentence_1, words_1[j], potential_replacement)\n",
    "        for j in indices_2:\n",
    "            potential_replacement = replace_word(words_2[j],list(set(clean_words(generate_homophones(words_2[j])))))\n",
    "            sentence_2 = put_word(sentence_2, words_2[j], potential_replacement)\n",
    "        df.loc[i, \"sentence 1\"] = sentence_1\n",
    "        df.loc[i, \"sentence 2\"] = sentence_2\n",
    "        print(f\"Iteration number {i}\")\n",
    "        df.to_excel('synthetised-dataset.xlsx')\n",
    "        \n",
    "    return df[[\"sentence 1\", \"sentence 2\", column_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2370\n",
      "Iteration number 2371\n",
      "Iteration number 2372\n",
      "Iteration number 2373\n",
      "Iteration number 2374\n",
      "Iteration number 2375\n",
      "Iteration number 2376\n",
      "Iteration number 2377\n",
      "Iteration number 2378\n",
      "Iteration number 2379\n",
      "Iteration number 2380\n",
      "Iteration number 2381\n",
      "Iteration number 2382\n",
      "Iteration number 2383\n",
      "Iteration number 2384\n",
      "Iteration number 2385\n",
      "Iteration number 2386\n",
      "Iteration number 2387\n",
      "Iteration number 2388\n",
      "Iteration number 2389\n",
      "Iteration number 2390\n",
      "Iteration number 2391\n",
      "Iteration number 2392\n",
      "Iteration number 2393\n",
      "Iteration number 2394\n",
      "Iteration number 2395\n",
      "Iteration number 2396\n",
      "Iteration number 2397\n",
      "Iteration number 2398\n",
      "Iteration number 2399\n",
      "Iteration number 2400\n",
      "Iteration number 2401\n",
      "Iteration number 2402\n",
      "Iteration number 2403\n",
      "Iteration number 2404\n",
      "Iteration number 2405\n",
      "Iteration number 2406\n",
      "Iteration number 2407\n",
      "Iteration number 2408\n",
      "Iteration number 2409\n",
      "Iteration number 2410\n",
      "Iteration number 2411\n",
      "Iteration number 2412\n",
      "Iteration number 2413\n",
      "Iteration number 2414\n",
      "Iteration number 2415\n",
      "Iteration number 2416\n",
      "Iteration number 2417\n",
      "Iteration number 2418\n",
      "Iteration number 2419\n",
      "Iteration number 2420\n",
      "Iteration number 2421\n",
      "Iteration number 2422\n",
      "Iteration number 2423\n",
      "Iteration number 2424\n",
      "Iteration number 2425\n",
      "Iteration number 2426\n",
      "Iteration number 2427\n",
      "Iteration number 2428\n",
      "Iteration number 2429\n",
      "Iteration number 2430\n",
      "Iteration number 2431\n",
      "Iteration number 2432\n",
      "Iteration number 2433\n",
      "Iteration number 2434\n",
      "Iteration number 2435\n",
      "Iteration number 2436\n",
      "Iteration number 2437\n",
      "Iteration number 2438\n",
      "Iteration number 2439\n",
      "Iteration number 2440\n",
      "Iteration number 2441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading names: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2442\n",
      "Iteration number 2443\n",
      "Iteration number 2444\n",
      "Iteration number 2445\n",
      "Iteration number 2446\n",
      "Iteration number 2447\n",
      "Iteration number 2448\n",
      "Iteration number 2449\n",
      "Iteration number 2450\n",
      "Iteration number 2451\n",
      "Iteration number 2452\n",
      "Iteration number 2453\n",
      "Iteration number 2454\n",
      "Iteration number 2455\n",
      "Iteration number 2456\n",
      "Iteration number 2457\n",
      "Iteration number 2458\n",
      "Iteration number 2459\n",
      "Iteration number 2460\n",
      "Iteration number 2461\n",
      "Iteration number 2462\n",
      "Iteration number 2463\n",
      "Iteration number 2464\n",
      "Iteration number 2465\n",
      "Iteration number 2466\n",
      "Iteration number 2467\n",
      "Iteration number 2468\n",
      "Iteration number 2469\n",
      "Iteration number 2470\n",
      "Iteration number 2471\n",
      "Iteration number 2472\n",
      "Iteration number 2473\n",
      "Iteration number 2474\n",
      "Iteration number 2475\n",
      "Iteration number 2476\n",
      "Iteration number 2477\n",
      "Iteration number 2478\n",
      "Iteration number 2479\n",
      "Iteration number 2480\n",
      "Iteration number 2481\n",
      "Iteration number 2482\n",
      "Iteration number 2483\n",
      "Iteration number 2484\n",
      "Iteration number 2485\n",
      "Iteration number 2486\n",
      "Iteration number 2487\n",
      "Iteration number 2488\n",
      "Iteration number 2489\n",
      "Iteration number 2490\n",
      "Iteration number 2491\n",
      "Iteration number 2492\n",
      "Iteration number 2493\n",
      "Iteration number 2494\n",
      "Iteration number 2495\n",
      "Iteration number 2496\n",
      "Iteration number 2497\n",
      "Iteration number 2498\n",
      "Iteration number 2499\n",
      "Iteration number 2500\n",
      "Iteration number 2501\n",
      "Iteration number 2502\n",
      "Iteration number 2503\n",
      "Iteration number 2504\n",
      "Iteration number 2505\n",
      "Iteration number 2506\n",
      "Iteration number 2507\n",
      "Iteration number 2508\n",
      "Iteration number 2509\n",
      "Iteration number 2510\n",
      "Iteration number 2511\n",
      "Iteration number 2512\n",
      "Iteration number 2513\n",
      "Iteration number 2514\n",
      "Iteration number 2515\n",
      "Iteration number 2516\n",
      "Iteration number 2517\n",
      "Iteration number 2518\n",
      "Iteration number 2519\n",
      "Iteration number 2520\n",
      "Iteration number 2521\n",
      "Iteration number 2522\n",
      "Iteration number 2523\n",
      "Iteration number 2524\n",
      "Iteration number 2525\n",
      "Iteration number 2526\n",
      "Iteration number 2527\n",
      "Iteration number 2528\n",
      "Iteration number 2529\n",
      "Iteration number 2530\n",
      "Iteration number 2531\n",
      "Iteration number 2532\n",
      "Iteration number 2533\n",
      "Iteration number 2534\n",
      "Iteration number 2535\n",
      "Iteration number 2536\n",
      "Iteration number 2537\n",
      "Iteration number 2538\n",
      "Iteration number 2539\n",
      "Iteration number 2540\n",
      "Iteration number 2541\n",
      "Iteration number 2542\n",
      "Iteration number 2543\n",
      "Iteration number 2544\n",
      "Iteration number 2545\n",
      "Iteration number 2546\n",
      "Iteration number 2547\n",
      "Iteration number 2548\n",
      "Iteration number 2549\n",
      "Iteration number 2550\n",
      "Iteration number 2551\n",
      "Iteration number 2552\n",
      "Iteration number 2553\n",
      "Iteration number 2554\n",
      "Iteration number 2555\n",
      "Iteration number 2556\n",
      "Iteration number 2557\n",
      "Iteration number 2558\n",
      "Iteration number 2559\n",
      "Iteration number 2560\n",
      "Iteration number 2561\n",
      "Iteration number 2562\n",
      "Iteration number 2563\n",
      "Iteration number 2564\n",
      "Iteration number 2565\n",
      "Iteration number 2566\n",
      "Iteration number 2567\n",
      "Iteration number 2568\n",
      "Iteration number 2569\n",
      "Iteration number 2570\n",
      "Iteration number 2571\n",
      "Iteration number 2572\n",
      "Iteration number 2573\n",
      "Iteration number 2574\n",
      "Iteration number 2575\n",
      "Iteration number 2576\n",
      "Iteration number 2577\n",
      "Iteration number 2578\n",
      "Iteration number 2579\n",
      "Iteration number 2580\n",
      "Iteration number 2581\n",
      "Iteration number 2582\n",
      "Iteration number 2583\n",
      "Iteration number 2584\n",
      "Iteration number 2585\n",
      "Iteration number 2586\n",
      "Iteration number 2587\n",
      "Iteration number 2588\n",
      "Iteration number 2589\n",
      "Iteration number 2590\n",
      "Iteration number 2591\n",
      "Iteration number 2592\n",
      "Iteration number 2593\n",
      "Iteration number 2594\n",
      "Iteration number 2595\n",
      "Iteration number 2596\n",
      "Iteration number 2597\n",
      "Iteration number 2598\n",
      "Iteration number 2599\n",
      "Iteration number 2600\n",
      "Iteration number 2601\n",
      "Iteration number 2602\n",
      "Iteration number 2603\n",
      "Iteration number 2604\n",
      "Iteration number 2605\n",
      "Iteration number 2606\n",
      "Iteration number 2607\n",
      "Iteration number 2608\n",
      "Iteration number 2609\n",
      "Iteration number 2610\n",
      "Iteration number 2611\n",
      "Iteration number 2612\n",
      "Iteration number 2613\n",
      "Iteration number 2614\n",
      "Iteration number 2615\n",
      "Iteration number 2616\n",
      "Iteration number 2617\n",
      "Iteration number 2618\n",
      "Iteration number 2619\n",
      "Iteration number 2620\n",
      "Iteration number 2621\n",
      "Iteration number 2622\n",
      "Iteration number 2623\n",
      "Iteration number 2624\n",
      "Iteration number 2625\n",
      "Iteration number 2626\n",
      "Iteration number 2627\n",
      "Iteration number 2628\n",
      "Iteration number 2629\n",
      "Iteration number 2630\n",
      "Iteration number 2631\n",
      "Iteration number 2632\n",
      "Iteration number 2633\n",
      "Iteration number 2634\n",
      "Iteration number 2635\n",
      "Iteration number 2636\n",
      "Iteration number 2637\n",
      "Iteration number 2638\n",
      "Iteration number 2639\n",
      "Iteration number 2640\n",
      "Iteration number 2641\n",
      "Iteration number 2642\n",
      "Iteration number 2643\n",
      "Iteration number 2644\n",
      "Iteration number 2645\n",
      "Iteration number 2646\n",
      "Iteration number 2647\n",
      "Iteration number 2648\n",
      "Iteration number 2649\n",
      "Iteration number 2650\n",
      "Iteration number 2651\n",
      "Iteration number 2652\n",
      "Iteration number 2653\n",
      "Iteration number 2654\n",
      "Iteration number 2655\n",
      "Iteration number 2656\n",
      "Iteration number 2657\n",
      "Iteration number 2658\n",
      "Iteration number 2659\n",
      "Iteration number 2660\n",
      "Iteration number 2661\n",
      "Iteration number 2662\n",
      "Iteration number 2663\n",
      "Iteration number 2664\n",
      "Iteration number 2665\n",
      "Iteration number 2666\n",
      "Iteration number 2667\n",
      "Iteration number 2668\n",
      "Iteration number 2669\n",
      "Iteration number 2670\n",
      "Iteration number 2671\n",
      "Iteration number 2672\n",
      "Iteration number 2673\n",
      "Iteration number 2674\n",
      "Iteration number 2675\n",
      "Iteration number 2676\n",
      "Iteration number 2677\n",
      "Iteration number 2678\n",
      "Iteration number 2679\n",
      "Iteration number 2680\n",
      "Iteration number 2681\n",
      "Iteration number 2682\n",
      "Iteration number 2683\n",
      "Iteration number 2684\n",
      "Iteration number 2685\n",
      "Iteration number 2686\n",
      "Iteration number 2687\n",
      "Iteration number 2688\n",
      "Iteration number 2689\n",
      "Iteration number 2690\n",
      "Iteration number 2691\n",
      "Iteration number 2692\n",
      "Iteration number 2693\n",
      "Iteration number 2694\n",
      "Iteration number 2695\n",
      "Iteration number 2696\n",
      "Iteration number 2697\n",
      "Iteration number 2698\n",
      "Iteration number 2699\n",
      "Iteration number 2700\n",
      "Iteration number 2701\n",
      "Iteration number 2702\n",
      "Iteration number 2703\n",
      "Iteration number 2704\n",
      "Iteration number 2705\n",
      "Iteration number 2706\n",
      "Iteration number 2707\n",
      "Iteration number 2708\n",
      "Iteration number 2709\n",
      "Iteration number 2710\n",
      "Iteration number 2711\n",
      "Iteration number 2712\n",
      "Iteration number 2713\n",
      "Iteration number 2714\n",
      "Iteration number 2715\n",
      "Iteration number 2716\n",
      "Iteration number 2717\n",
      "Iteration number 2718\n",
      "Iteration number 2719\n",
      "Iteration number 2720\n",
      "Iteration number 2721\n",
      "Iteration number 2722\n",
      "Iteration number 2723\n",
      "Iteration number 2724\n",
      "Iteration number 2725\n",
      "Iteration number 2726\n",
      "Iteration number 2727\n",
      "Iteration number 2728\n",
      "Iteration number 2729\n",
      "Iteration number 2730\n",
      "Iteration number 2731\n",
      "Iteration number 2732\n",
      "Iteration number 2733\n",
      "Iteration number 2734\n",
      "Iteration number 2735\n",
      "Iteration number 2736\n",
      "Iteration number 2737\n",
      "Iteration number 2738\n",
      "Iteration number 2739\n",
      "Iteration number 2740\n",
      "Iteration number 2741\n",
      "Iteration number 2742\n",
      "Iteration number 2743\n",
      "Iteration number 2744\n",
      "Iteration number 2745\n",
      "Iteration number 2746\n",
      "Iteration number 2747\n",
      "Iteration number 2748\n",
      "Iteration number 2749\n",
      "Iteration number 2750\n",
      "Iteration number 2751\n",
      "Iteration number 2752\n",
      "Iteration number 2753\n",
      "Iteration number 2754\n",
      "Iteration number 2755\n",
      "Iteration number 2756\n",
      "Iteration number 2757\n",
      "Iteration number 2758\n",
      "Iteration number 2759\n",
      "Iteration number 2760\n",
      "Iteration number 2761\n",
      "Iteration number 2762\n",
      "Iteration number 2763\n",
      "Iteration number 2764\n",
      "Iteration number 2765\n",
      "Iteration number 2766\n",
      "Iteration number 2767\n",
      "Iteration number 2768\n",
      "Iteration number 2769\n",
      "Iteration number 2770\n",
      "Iteration number 2771\n",
      "Iteration number 2772\n",
      "Iteration number 2773\n",
      "Iteration number 2774\n",
      "Iteration number 2775\n",
      "Iteration number 2776\n",
      "Iteration number 2777\n",
      "Iteration number 2778\n",
      "Iteration number 2779\n",
      "Iteration number 2780\n",
      "Iteration number 2781\n",
      "Iteration number 2782\n",
      "Iteration number 2783\n",
      "Iteration number 2784\n",
      "Iteration number 2785\n",
      "Iteration number 2786\n",
      "Iteration number 2787\n",
      "Iteration number 2788\n",
      "Iteration number 2789\n",
      "Iteration number 2790\n",
      "Iteration number 2791\n",
      "Iteration number 2792\n",
      "Iteration number 2793\n",
      "Iteration number 2794\n",
      "Iteration number 2795\n",
      "Iteration number 2796\n",
      "Iteration number 2797\n",
      "Iteration number 2798\n",
      "Iteration number 2799\n",
      "Iteration number 2800\n",
      "Iteration number 2801\n",
      "Iteration number 2802\n",
      "Iteration number 2803\n",
      "Iteration number 2804\n",
      "Iteration number 2805\n",
      "Iteration number 2806\n",
      "Iteration number 2807\n",
      "Iteration number 2808\n",
      "Iteration number 2809\n",
      "Iteration number 2810\n",
      "Iteration number 2811\n",
      "Iteration number 2812\n",
      "Iteration number 2813\n",
      "Iteration number 2814\n",
      "Iteration number 2815\n",
      "Iteration number 2816\n",
      "Iteration number 2817\n",
      "Iteration number 2818\n",
      "Iteration number 2819\n",
      "Iteration number 2820\n",
      "Iteration number 2821\n",
      "Iteration number 2822\n",
      "Iteration number 2823\n",
      "Iteration number 2824\n",
      "Iteration number 2825\n",
      "Iteration number 2826\n",
      "Iteration number 2827\n",
      "Iteration number 2828\n",
      "Iteration number 2829\n",
      "Iteration number 2830\n",
      "Iteration number 2831\n",
      "Iteration number 2832\n",
      "Iteration number 2833\n",
      "Iteration number 2834\n",
      "Iteration number 2835\n",
      "Iteration number 2836\n",
      "Iteration number 2837\n",
      "Iteration number 2838\n",
      "Iteration number 2839\n",
      "Iteration number 2840\n",
      "Iteration number 2841\n",
      "Iteration number 2842\n",
      "Iteration number 2843\n",
      "Iteration number 2844\n",
      "Iteration number 2845\n",
      "Iteration number 2846\n",
      "Iteration number 2847\n",
      "Iteration number 2848\n",
      "Iteration number 2849\n",
      "Iteration number 2850\n",
      "Iteration number 2851\n",
      "Iteration number 2852\n",
      "Iteration number 2853\n",
      "Iteration number 2854\n",
      "Iteration number 2855\n",
      "Iteration number 2856\n",
      "Iteration number 2857\n",
      "Iteration number 2858\n",
      "Iteration number 2859\n",
      "Iteration number 2860\n",
      "Iteration number 2861\n",
      "Iteration number 2862\n",
      "Iteration number 2863\n",
      "Iteration number 2864\n",
      "Iteration number 2865\n",
      "Iteration number 2866\n",
      "Iteration number 2867\n",
      "Iteration number 2868\n",
      "Iteration number 2869\n",
      "Iteration number 2870\n",
      "Iteration number 2871\n",
      "Iteration number 2872\n",
      "Iteration number 2873\n",
      "Iteration number 2874\n",
      "Iteration number 2875\n",
      "Iteration number 2876\n",
      "Iteration number 2877\n",
      "Iteration number 2878\n",
      "Iteration number 2879\n",
      "Iteration number 2880\n",
      "Iteration number 2881\n",
      "Iteration number 2882\n",
      "Iteration number 2883\n",
      "Iteration number 2884\n",
      "Iteration number 2885\n",
      "Iteration number 2886\n",
      "Iteration number 2887\n",
      "Iteration number 2888\n",
      "Iteration number 2889\n",
      "Iteration number 2890\n",
      "Iteration number 2891\n",
      "Iteration number 2892\n",
      "Iteration number 2893\n",
      "Iteration number 2894\n",
      "Iteration number 2895\n",
      "Iteration number 2896\n",
      "Iteration number 2897\n",
      "Iteration number 2898\n",
      "Iteration number 2899\n",
      "Iteration number 2900\n",
      "Iteration number 2901\n",
      "Iteration number 2902\n",
      "Iteration number 2903\n",
      "Iteration number 2904\n",
      "Iteration number 2905\n",
      "Iteration number 2906\n",
      "Iteration number 2907\n",
      "Iteration number 2908\n",
      "Iteration number 2909\n",
      "Iteration number 2910\n",
      "Iteration number 2911\n",
      "Iteration number 2912\n",
      "Iteration number 2913\n",
      "Iteration number 2914\n",
      "Iteration number 2915\n",
      "Iteration number 2916\n",
      "Iteration number 2917\n",
      "Iteration number 2918\n",
      "Iteration number 2919\n",
      "Iteration number 2920\n",
      "Iteration number 2921\n",
      "Iteration number 2922\n",
      "Iteration number 2923\n",
      "Iteration number 2924\n",
      "Iteration number 2925\n",
      "Iteration number 2926\n",
      "Iteration number 2927\n",
      "Iteration number 2928\n",
      "Iteration number 2929\n",
      "Iteration number 2930\n",
      "Iteration number 2931\n",
      "Iteration number 2932\n",
      "Iteration number 2933\n",
      "Iteration number 2934\n",
      "Iteration number 2935\n",
      "Iteration number 2936\n",
      "Iteration number 2937\n",
      "Iteration number 2938\n",
      "Iteration number 2939\n",
      "Iteration number 2940\n",
      "Iteration number 2941\n",
      "Iteration number 2942\n",
      "Iteration number 2943\n",
      "Iteration number 2944\n",
      "Iteration number 2945\n",
      "Iteration number 2946\n",
      "Iteration number 2947\n",
      "Iteration number 2948\n",
      "Iteration number 2949\n",
      "Iteration number 2950\n",
      "Iteration number 2951\n",
      "Iteration number 2952\n",
      "Iteration number 2953\n",
      "Iteration number 2954\n",
      "Iteration number 2955\n",
      "Iteration number 2956\n",
      "Iteration number 2957\n",
      "Iteration number 2958\n",
      "Iteration number 2959\n",
      "Iteration number 2960\n",
      "Iteration number 2961\n",
      "Iteration number 2962\n",
      "Iteration number 2963\n",
      "Iteration number 2964\n",
      "Iteration number 2965\n",
      "Iteration number 2966\n",
      "Iteration number 2967\n",
      "Iteration number 2968\n",
      "Iteration number 2969\n",
      "Iteration number 2970\n",
      "Iteration number 2971\n",
      "Iteration number 2972\n",
      "Iteration number 2973\n",
      "Iteration number 2974\n",
      "Iteration number 2975\n",
      "Iteration number 2976\n",
      "Iteration number 2977\n",
      "Iteration number 2978\n",
      "Iteration number 2979\n",
      "Iteration number 2980\n",
      "Iteration number 2981\n",
      "Iteration number 2982\n",
      "Iteration number 2983\n",
      "Iteration number 2984\n",
      "Iteration number 2985\n",
      "Iteration number 2986\n",
      "Iteration number 2987\n",
      "Iteration number 2988\n",
      "Iteration number 2989\n",
      "Iteration number 2990\n",
      "Iteration number 2991\n",
      "Iteration number 2992\n",
      "Iteration number 2993\n",
      "Iteration number 2994\n",
      "Iteration number 2995\n",
      "Iteration number 2996\n",
      "Iteration number 2997\n",
      "Iteration number 2998\n",
      "Iteration number 2999\n",
      "Iteration number 3000\n",
      "Iteration number 3001\n",
      "Iteration number 3002\n",
      "Iteration number 3003\n",
      "Iteration number 3004\n",
      "Iteration number 3005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_excel(\u001b[39m'\u001b[39m\u001b[39mdataset.xlsx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m data\u001b[39m.\u001b[39mdrop(data\u001b[39m.\u001b[39mcolumns[\u001b[39m0\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m datadf \u001b[39m=\u001b[39m generate_dataset(data,data\u001b[39m.\u001b[39;49mcolumns[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m      5\u001b[0m datadf\u001b[39m.\u001b[39mto_excel(\u001b[39m\"\u001b[39m\u001b[39msynthetised-dataset.xlsx\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[1;34m(df, column_name)\u001b[0m\n\u001b[0;32m     21\u001b[0m indices_2 \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(words_2)) \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m indices_1], m)\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m indices_1:\n\u001b[1;32m---> 23\u001b[0m     potential_replacement \u001b[39m=\u001b[39m replace_word(words_1[j],\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(clean_words(generate_homophones(words_1[j])))))\n\u001b[0;32m     24\u001b[0m     sentence_1 \u001b[39m=\u001b[39m put_word(sentence_1, words_1[j], potential_replacement)\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m indices_2:\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mclean_words\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[0;32m      7\u001b[0m     clean_word \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^a-zA-Z]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, word)\n\u001b[1;32m----> 8\u001b[0m     \u001b[39mif\u001b[39;00m clean_word \u001b[39mand\u001b[39;00m is_english_word_or_name(word):\n\u001b[0;32m      9\u001b[0m         clean_words\u001b[39m.\u001b[39mappend(clean_word)\n\u001b[0;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m [w\u001b[39m.\u001b[39mupper() \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m clean_words]\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mis_english_word_or_name\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Get a set of English words and names\u001b[39;00m\n\u001b[0;32m      7\u001b[0m english_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mwords\u001b[39m.\u001b[39mwords())\n\u001b[1;32m----> 8\u001b[0m english_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mnames\u001b[39m.\u001b[39mwords())\n\u001b[0;32m     10\u001b[0m \u001b[39m# Check if the word is in the set of English words or names\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m english_words \u001b[39mor\u001b[39;00m word\u001b[39m.\u001b[39mtitle() \u001b[39min\u001b[39;00m english_names:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('dataset.xlsx')\n",
    "data.drop(data.columns[0], axis=1, inplace=True)\n",
    "datadf = generate_dataset(data,data.columns[0])\n",
    "datadf.to_excel(\"synthetised-dataset.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche 4: Amelioration de l'approche 1\n",
    "## Steps :\n",
    "### Target is just the copy of the correct sentence :\n",
    "### Sentence 1 is a copy of the target with random homophones on randon spots\n",
    "### Sentence 2 is the Target, first decomposed to morphemes, THEN replaced with homphones\n",
    "#### Disclamer : A morpheme example : morpheme(unhappy)--> un-happy\n",
    "#### Utility : Evaluating the case where we have bi-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the required nltk modules\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to segment a sentence into morphemes\n",
    "def segment_sentence(sentence):\n",
    "    # Tokenize \n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # POS TAGGING to get verbs et d'autres stuff\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    \n",
    "    # Segmentation using WordNet\n",
    "    morphemes = []\n",
    "    for word, tag in tagged_words:\n",
    "        if tag.startswith('NN'):  # Nouns\n",
    "            segments = nltk.corpus.wordnet.morphy(word, nltk.corpus.wordnet.NOUN)\n",
    "        elif tag.startswith('VB'):  # Verbs\n",
    "            segments = nltk.corpus.wordnet.morphy(word, nltk.corpus.wordnet.VERB)\n",
    "        elif tag.startswith('JJ'):  # Adjectives\n",
    "            segments = nltk.corpus.wordnet.morphy(word, nltk.corpus.wordnet.ADJ)\n",
    "        elif tag.startswith('RB'):  # Adverbs\n",
    "            segments = nltk.corpus.wordnet.morphy(word, nltk.corpus.wordnet.ADV)\n",
    "        else:\n",
    "            segments = [word]  # Si on n'a pas un tag on garde le mot tel qu'il est\n",
    "        \n",
    "        # Nzidou the morpheme segments \n",
    "        if segments is not None:  # check if segments is not None\n",
    "        # Nzidou the morpheme segments \n",
    "            morphemes.extend(segments)\n",
    "    \n",
    "    # Return the list of morphemes as a string parce qu'on a besoin de la phrase\n",
    "    return ' '.join(morphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wikisent2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Load the sentences from the local file\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mwikisent2.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m     sentences \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39msplitlines()\n\u001b[0;32m     11\u001b[0m \u001b[39m# Create a pandas dataframe with three columns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rania\\anaconda3\\envs\\mybase\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wikisent2.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Load the sentences from the local file\n",
    "with open('wikisent2.txt', 'r', encoding='utf-8') as f:\n",
    "    sentences = f.read().splitlines()\n",
    "\n",
    "# Create a pandas dataframe with three columns\n",
    "data = pd.DataFrame(columns=['sentence1', 'sentence2', 'target'])\n",
    "\n",
    "# Iterate through the sentences and create new rows in the dataframe\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # Create a new row with the original sentence in all three columns\n",
    "    row = {'sentence1': sentence, 'sentence2': sentence, 'target': sentence}\n",
    "    \n",
    "    # Split the sentence into words\n",
    "    words1 = sentence.split()\n",
    "    words2 = segment_sentence(sentence).split()\n",
    "    print(f\"Iteration number {i}\")\n",
    "    \n",
    "    # Iterate through the words and replace them randomly in either sentence1 or sentence2\n",
    "    for i, word in enumerate(words1):\n",
    "        if re.match(\"^[a-zA-Z]*$\", word):\n",
    "            potential_replacement = replace_word(word,list(set(clean_words(generate_homophones(word)))))\n",
    "            if potential_replacement is not None:\n",
    "                \n",
    "                if i % 2 == 0:\n",
    "                    row['sentence1'] = row['sentence1'].replace(word, potential_replacement)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                row['sentence1'] = row['sentence1']\n",
    "                \n",
    "        else:\n",
    "            row['sentence1'] = row['sentence1']\n",
    "            \n",
    "        data.to_csv('dataset.csv')\n",
    "        \n",
    "    for i, word in enumerate(words2):\n",
    "        if re.match(\"^[a-zA-Z]*$\", word):\n",
    "            potential_replacement = replace_word(word,list(set(clean_words(generate_homophones(word)))))\n",
    "            if potential_replacement is not None:\n",
    "                \n",
    "                if i % 2 == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    row['sentence2'] = row['sentence2'].replace(word, potential_replacement)\n",
    "            else:\n",
    "                row['sentence2'] = row['sentence2']\n",
    "        else:\n",
    "            row['sentence2'] = row['sentence2']\n",
    "        data.to_csv('dataset-morph.csv')\n",
    "    \n",
    "    # Append the new row to the dataframe\n",
    "    data = data.append(row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

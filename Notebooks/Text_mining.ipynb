{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Text Preprocessing using text mining</h1></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Tasks :</u></h3> \n",
    "<h4>- Text Cleaning</h4> \n",
    "<h4>- Text similarity evaluation</h4> \n",
    "<h4>- Extraction of common and uncommon parts</h4> \n",
    "<h4>- N gram distribution</h4>  \n",
    "<h4>- Apply the mask on the uncommon parts</h4> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\n",
    "from nltk.util import ngrams\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the sentences to preprocess\n",
    "\n",
    "sentence1 = \"I love to pay my video games in my free time, especially retro video games.\"\n",
    "sentence2 = \"I love to play oreo games in my free thyme, especially retro video games.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I love to pay my video games in my free time especially retro video games',\n",
       " 'I love to play oreo games in my free thyme especially retro video games')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "sentence1 = re.sub(r'[^\\w\\s]', '', sentence1)\n",
    "sentence2 = re.sub(r'[^\\w\\s]', '', sentence2)\n",
    "sentence1, sentence2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Text similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low priority task will be done using cosine similarity for a starter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Extraction of common and uncommon parts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_and_uncommon_extraction(str1, str2, len1, len2):\n",
    "    # initialize the L matrix with zeros\n",
    "    L = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n",
    "\n",
    "    # fill in the L matrix using dynamic programming\n",
    "    for i in range(len1 + 1):\n",
    "        for j in range(len2 + 1):\n",
    "            # if either string is empty, the longest common substring is zero\n",
    "            if i == 0 or j == 0:\n",
    "                L[i][j] = 0\n",
    "            # if the characters match, add one to the length of the longest common substring\n",
    "            elif str1[i - 1] == str2[j - 1]:\n",
    "                L[i][j] = L[i - 1][j - 1] + 1\n",
    "            # if the characters don't match, take the maximum length from the previous row or column\n",
    "            else:\n",
    "                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "\n",
    "    # calculate the index based on the length of the longer string\n",
    "    index = max(len(str1), len(str2))\n",
    "\n",
    "    # initialize the common list with empty strings\n",
    "    common = [\"\"] * (index + 1)\n",
    "    common[index] = \"\"\n",
    "\n",
    "    # initialize the uncommon substring lists\n",
    "    uncommon_str1 = []\n",
    "    uncommon_str2 = []\n",
    "\n",
    "    # set i and j to the end of each string\n",
    "    i = len1\n",
    "    j = len2\n",
    "\n",
    "    tracker_str1 = -1\n",
    "    tracker_str2 = -1\n",
    "    sub_uncommon_str1 = []\n",
    "    sub_uncommon_str2 = []\n",
    "\n",
    "    # loop through the L matrix to find the common and uncommon substrings\n",
    "    while i > 0 and j > 0:\n",
    "        # if the characters match, add the character to the common list and move to the previous diagonal cell\n",
    "        if str1[i - 1] == str2[j - 1]:\n",
    "            common[index - 1] = str1[i - 1]\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            index -= 1\n",
    "        # if the length of the substring from the previous row is greater, add the uncommon character to uncommon_str2 list and move to the previous row\n",
    "        elif L[i - 1][j] > L[i][j - 1]:\n",
    "            if tracker_str2 == -1:\n",
    "                tracker_str2 = i - 1\n",
    "                common[index - 1] = \"#\"\n",
    "                index -= 1\n",
    "                sub_uncommon_str2.append(str1[i - 1])\n",
    "            elif tracker_str2 == i:\n",
    "                sub_uncommon_str2.append(str1[i - 1])\n",
    "                tracker_str2 = i - 1\n",
    "            else:\n",
    "                sub_uncommon_str2.reverse()\n",
    "                uncommon_str2.append(sub_uncommon_str2)\n",
    "                sub_uncommon_str2 = []\n",
    "                tracker_str2 = i - 1\n",
    "                common[index - 1] = \"#\"\n",
    "                index -= 1\n",
    "                sub_uncommon_str2.append(str1[i - 1])\n",
    "\n",
    "            i -= 1\n",
    "        # if the length of the substring from the previous column is greater, add the uncommon character to uncommon_str1 list and move to the previous column\n",
    "        else:\n",
    "            if tracker_str1 == -1: \n",
    "                tracker_str1 = j - 1\n",
    "                sub_uncommon_str1.append(str2[j - 1])\n",
    "            elif tracker_str1 == j:\n",
    "                sub_uncommon_str1.append(str2[j - 1])\n",
    "                racker_str1 = j - 1\n",
    "            else:\n",
    "                sub_uncommon_str1.reverse()\n",
    "                uncommon_str1.append(sub_uncommon_str1)\n",
    "                sub_uncommon_str1 = []\n",
    "                tracker_str1 = j - 1\n",
    "                sub_uncommon_str1.append(str2[j - 1])\n",
    "\n",
    "            j -= 1\n",
    "\n",
    "    if len(sub_uncommon_str1) > 0:\n",
    "        sub_uncommon_str1.reverse()\n",
    "        uncommon_str1.append(sub_uncommon_str1)\n",
    "    if len(sub_uncommon_str2) > 0:\n",
    "        sub_uncommon_str2.reverse()\n",
    "        uncommon_str2.append(sub_uncommon_str2)\n",
    "\n",
    "    # join the common list into a sentence\n",
    "    common_sentence = \" \".join(common[2:-1])\n",
    "\n",
    "    # reverse the order of the uncommon substring lists\n",
    "    uncommon_str1.reverse()\n",
    "    uncommon_str2.reverse()\n",
    "\n",
    "    # return the common sentence and the lists of uncommon substrings\n",
    "    return common_sentence, uncommon_str1, uncommon_str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 : I love to pay my video games in my free time especially retro video games\n",
      "\n",
      "Sentence 2 : I love to play oreo games in my free thyme especially retro video games\n",
      "\n",
      "Common parts of the 2 sentences with the uncommon parts masked :\n",
      "I love to # games in my free # especially retro video games\n",
      "\n",
      "Uncommon parts of sentence 1 : [['play', 'oreo'], ['thyme']]\n",
      "\n",
      "Uncommon parts of sentence 2 : [['pay', 'my', 'video'], ['time']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence 1 : \"+ sentence1)\n",
    "print(\"\\nSentence 2 : \"+ sentence2)\n",
    "\n",
    "common_sentence, uncommon_str1, uncommon_str2 = common_and_uncommon_extraction(sentence1.split(), \n",
    "                                                                            sentence2.split(), \n",
    "                                                                            len(sentence1.split()),\n",
    "                                                                            len(sentence2.split()))\n",
    "\n",
    "print(\"\\nCommon parts of the 2 sentences with the uncommon parts masked :\\n\"+common_sentence) \n",
    "print(\"\\nUncommon parts of sentence 1 : \"+ str(uncommon_str1))\n",
    "print(\"\\nUncommon parts of sentence 2 : \"+ str(uncommon_str2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) N gram distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Finding the most fit value of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In progress"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) finding the best N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_distribution(uncommon_str1, uncommon_str2, sentence1, sentence2):\n",
    "    # Initialize empty lists to store the final best uncommon n-grams for each string\n",
    "    final_uncommon_str1 = []\n",
    "    final_uncommon_str2 = []\n",
    "    \n",
    "    # Iterate through the uncommon parts for each string\n",
    "    for i in range(len(uncommon_str1)):\n",
    "        # Check if the lengths of the current uncommon part for each string are different\n",
    "        if len(uncommon_str1[i]) != len(uncommon_str2[i]):\n",
    "            # If the lengths are different, create a list of all potential bigrams from sentence1 and sentence2\n",
    "            n = 2\n",
    "            bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "            # Find the best uncommon bigrams for uncommon_str1 in case the length of the uncommon part is greater than 2\n",
    "            if len(uncommon_str1[i]) > 2:\n",
    "                # Make a copy of the current list of the current uncommon part for string 1\n",
    "                uncommon_str1_i = uncommon_str1[i].copy()\n",
    "\n",
    "                # Variable containing the common words that won't allowed in the bigrams\n",
    "                common_words_str1 = list(set(sentence1.split()) - set(uncommon_str1_i))\n",
    "\n",
    "                # Tokenize the sentence\n",
    "                tokens_str1 = nltk.word_tokenize(sentence1)\n",
    "\n",
    "                # Generate a list of all n-grams of size n for the sentence\n",
    "                n_grams_str1 = list(ngrams(tokens_str1, n))\n",
    "                \n",
    "                # Use the bigram collocation finder to get the best bigrams for the sentence\n",
    "                finder_str1 = BigramCollocationFinder.from_words(tokens_str1)\n",
    "                best_bigrams_str1 = finder.nbest(bigram_measures.pmi, len(n_grams_str1))\n",
    "\n",
    "                # Filter out bigrams that contain common words from the current list of uncommon words\n",
    "                best_uncommon_ngrams_str1 = [ngram for ngram in best_bigrams_str1 if (not any(p_ngrams in ngram for p_ngrams in common_words_str1))]\n",
    "                \n",
    "                # Generate the final list of uncommon n-grams for string 1 by filtering the filtered bigrams and remaining uncommon words\n",
    "                uncommon_ngrams_str1 = []\n",
    "                for i in best_uncommon_ngrams_str1:\n",
    "                    if i[0] in uncommon_str1_i and i[1] in uncommon_str1_i:\n",
    "                        uncommon_ngrams_str1.append(\" \".join(list(i)))\n",
    "                        uncommon_str1_i.remove(i[0])\n",
    "                        uncommon_str1_i.remove(i[1])\n",
    "                if uncommon_str1_i != []:\n",
    "                    uncommon_ngrams_str1.extend(uncommon_str1_i)\n",
    "\n",
    "                final_uncommon_str1.append(uncommon_ngrams_str1) # Append the final uncommon n-grams for string 1 to the list\n",
    "            else :\n",
    "                # If the length of the current uncommon words list for string 1 is 2 or less, append the current list to the final list\n",
    "                final_uncommon_str1.append(uncommon_str1[i])\n",
    "\n",
    "            if len(uncommon_str2[i]) > 2:\n",
    "                # Make a copy of the current list of the current uncommon part for string 2\n",
    "                uncommon_str2_i = uncommon_str2[i].copy()\n",
    "\n",
    "                # Variable containing the common words that won't allowed in the bigrams\n",
    "                common_words_str2 = list(set(sentence2.split()) - set(uncommon_str2_i))\n",
    "                \n",
    "                # Tokenize the sentence\n",
    "                tokens_str2 = nltk.word_tokenize(sentence2)\n",
    "                \n",
    "                # Generate a list of all n-grams of size n for the sentence\n",
    "                n_grams_str2 = list(ngrams(tokens_str2, n))\n",
    "                \n",
    "                # Use the bigram collocation finder to get the best bigrams for the sentence\n",
    "                finder_str2 = BigramCollocationFinder.from_words(tokens_str2)\n",
    "                best_bigrams_str2 = finder.nbest(bigram_measures.pmi, len(n_grams_str2))\n",
    "\n",
    "                # Filter out bigrams that contain common words from the current list of uncommon words\n",
    "                best_uncommon_ngrams_str2 = [ngram for ngram in best_bigrams_str2 if (not any(p_ngrams in ngram for p_ngrams in common_words_str2))]\n",
    "                \n",
    "                # Generate the final list of uncommon n-grams for string 2 by filtering the filtered bigrams and remaining uncommon words\n",
    "                uncommon_ngrams_str2 = []\n",
    "                for i in best_uncommon_ngrams_str2:\n",
    "                    if i[0] in uncommon_str2_i and i[1] in uncommon_str2_i:\n",
    "                        uncommon_ngrams_str2.append(\" \".join(list(i)))\n",
    "                        uncommon_str2_i.remove(i[0])\n",
    "                        uncommon_str2_i.remove(i[1])\n",
    "                if uncommon_str2_i != []:\n",
    "                    uncommon_ngrams_str2.extend(uncommon_str2_i)\n",
    "\n",
    "                final_uncommon_str2.append(uncommon_ngrams_str2) # Append the final uncommon n-grams for string 2 to the list\n",
    "            else :\n",
    "                # If the length of the current uncommon words list for string 2 is 2 or less, append the current list to the list\n",
    "                final_uncommon_str2.append(uncommon_str2[i])\n",
    "\n",
    "        else:\n",
    "            # If the lengths of the current uncommon parts for each string are the same, append the current lists to the final lists\n",
    "            final_uncommon_str1.append(uncommon_str1[i])\n",
    "            final_uncommon_str2.append(uncommon_str2[i])\n",
    "\n",
    "    return final_uncommon_str1, final_uncommon_str2\n",
    "\n",
    "final_uncommon_str1, final_uncommon_str2 = ngram_distribution(uncommon_str1, uncommon_str2, sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['play', 'oreo'], ['thyme']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_uncommon_str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pay my', 'video'], ['time']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_uncommon_str2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Apply the mask on the uncommon parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking(common_sentence, uncommon_str):\n",
    "    for i in range(len(uncommon_str)):\n",
    "        mask = \"[MASK] \" * len(uncommon_str[i])\n",
    "        common_sentence = common_sentence.replace(\"#\", mask, 1)\n",
    "    return \" \".join(common_sentence.split())\n",
    "masked_sentence = masking(common_sentence, final_uncommon_str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 : I love to pay my video games in my free time especially retro video games\n",
      "\n",
      "Sentence 2 : I love to play oreo games in my free thyme especially retro video games\n",
      "\n",
      "Common parts of the 2 sentences with the uncommon parts masked :\n",
      "I love to [MASK] [MASK] games in my free [MASK] especially retro video games\n",
      "\n",
      "Uncommon parts of sentence 1 : [['play', 'oreo'], ['thyme']]\n",
      "\n",
      "Uncommon parts of sentence 2 : [['pay my', 'video'], ['time']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence 1 : \"+ sentence1)\n",
    "print(\"\\nSentence 2 : \"+ sentence2)\n",
    "print(\"\\nCommon parts of the 2 sentences with the uncommon parts masked :\\n\"+ masked_sentence) \n",
    "print(\"\\nUncommon parts of sentence 1 : \"+ str(final_uncommon_str1))\n",
    "print(\"\\nUncommon parts of sentence 2 : \"+ str(final_uncommon_str2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

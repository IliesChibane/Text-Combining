{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Text Preprocessing using text mining</h1></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Tasks :</u></h3> \n",
    "<h4>- Text Cleaning</h4> \n",
    "<h4>- Text similarity evaluation</h4> \n",
    "<h4>- Extraction of common and uncommon parts</h4> \n",
    "<h4>- N gram distribution</h4>  \n",
    "<h4>- Apply the mask on the uncommon parts</h4> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\n",
    "from nltk.util import ngrams\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the sentences to preprocess\n",
    "\n",
    "sentence1 = \"I love to pay my video games in my free time, especially retro video games.\"\n",
    "sentence2 = \"I love to play oreo games in my free thyme, especially retro video games.\"\n",
    "sentence3 = \"I live to slay video vames in my free time, especially utro video games.\"\n",
    "sentences = np.array([[sentence1], [sentence2], [sentence3]])\n",
    "# sentence1 = \"hello my name is james and I work as a data scientists\"\n",
    "# sentence2 = \"hello my nail fits jane andy workable scientist\"\n",
    "# sentences = np.array([[sentence1], [sentence2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['i love to pay my video games in my free time especially retro video games'],\n",
       "       ['i love to play oreo games in my free thyme especially retro video games'],\n",
       "       ['i live to slay video vames in my free time especially utro video games']],\n",
       "      dtype='<U73')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence[0]\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return [sentence]\n",
    "sentences = np.apply_along_axis(preprocess, 1, sentences)\n",
    "sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Text similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low priority task will be done using cosine similarity for a starter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Extraction of common and uncommon parts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the sentences into vectors\n",
    "def split_preprocess(sentences):\n",
    "    s = []\n",
    "    for sentence in sentences:\n",
    "        s.append(sentence[0].split())\n",
    "    return s\n",
    "\n",
    "# Reduce the sequences of # into one #\n",
    "def shrink(sentence):\n",
    "    temp = sentence.split()\n",
    "    b = False\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] == \"#\" and b:\n",
    "            temp[i] = \"\"\n",
    "        elif temp[i] == \"#\" and not b:\n",
    "            b = True\n",
    "        elif temp[i] != \"#\" and b:\n",
    "            b = False\n",
    "    while \"\" in temp:       \n",
    "        temp.remove(\"\")\n",
    "    \n",
    "    return \" \".join(temp)\n",
    "\n",
    "# Init the Dynamic matrix\n",
    "def init_matrix(temp_sentence, sentences, lenght, l):\n",
    "        # initialize the L matrix with zeros\n",
    "        L = [[0] * (lenght + 1) for _ in range(len(temp_sentence) + 1)]\n",
    "\n",
    "        # fill in the L matrix using dynamic programming\n",
    "        for i in range(len(temp_sentence) + 1):\n",
    "            for j in range(lenght + 1):\n",
    "                # if either string is empty, the longest common substring is zero\n",
    "                if i == 0 or j == 0:\n",
    "                    L[i][j] = 0\n",
    "                # if the characters match, add one to the length of the longest common substring\n",
    "                elif temp_sentence[i - 1] == sentences[l][j - 1]:\n",
    "                    L[i][j] = L[i - 1][j - 1] + 1\n",
    "                # if the characters don't match, take the maximum length from the previous row or column\n",
    "                else:\n",
    "                    L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "        return L\n",
    "\n",
    "# init list of lists\n",
    "def init_list_of_lists(lenght):\n",
    "    list_of_lists = []\n",
    "    for i in range(lenght):\n",
    "        list_of_lists.append([])\n",
    "    return list_of_lists\n",
    "\n",
    "# remove all the occourences of a value in a list\n",
    "def remove_all(liste, value):\n",
    "    while value in liste:\n",
    "        liste.remove(value)\n",
    "    return liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_and_uncommon_extraction(sentences):\n",
    "    lens = [len(s) for s in sentences]\n",
    "\n",
    "    # initialize the uncommon substring lists\n",
    "    uncommon_str_i = init_list_of_lists(len(sentences))\n",
    "\n",
    "    temp_sentence = sentences[0]\n",
    "    for l in range(1, len(sentences)):\n",
    "        # initialize the L matrix\n",
    "        L = init_matrix(temp_sentence, sentences, lens[l], l)\n",
    "\n",
    "        # calculate the index based on the length of the longer string\n",
    "        index = max(len(temp_sentence), lens[l])\n",
    "\n",
    "        # initialize the common list with empty strings\n",
    "        common = [\"\"] * (index + 1)\n",
    "        common[index] = \"\"\n",
    "\n",
    "        # set i and j to the end of each string\n",
    "        i = len(temp_sentence)\n",
    "        j = lens[l]\n",
    "\n",
    "        tracker_str1 = -1\n",
    "        tracker_str2 = -1\n",
    "        sub_uncommon_str = []\n",
    "        sub_uncommon = []\n",
    "        sub_uncommon_str_i_temp = []\n",
    "        sub_uncommon_str_temp = init_list_of_lists(len(sentences))\n",
    "\n",
    "        # loop through the L matrix to find the common and uncommon substrings\n",
    "        while i > 0 and j > 0:\n",
    "            # if the characters match, add the character to the common list and move to the previous diagonal cell\n",
    "            if temp_sentence[i - 1] == sentences[l][j - 1]:\n",
    "                common[index - 1] = temp_sentence[i - 1]\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "                index -= 1\n",
    "            # if the length of the substring from the previous column is greater, add the uncommon character to uncommon_str1 list and move to the previous column\n",
    "            elif L[i - 1][j] < L[i][j - 1]:\n",
    "                if tracker_str1 == -1: \n",
    "                    tracker_str1 = j - 1\n",
    "                    sub_uncommon_str.append(sentences[l][j - 1])\n",
    "                elif tracker_str1 == j:\n",
    "                    sub_uncommon_str.append(sentences[l][j - 1])\n",
    "                    tracker_str1 = j - 1\n",
    "                else:\n",
    "                    sub_uncommon_str.reverse()\n",
    "                    uncommon_str_i[l].append(sub_uncommon_str if len(sub_uncommon_str) > 1 else sub_uncommon_str[0])\n",
    "                    sub_uncommon_str = []\n",
    "                    tracker_str1 = j - 1\n",
    "                    sub_uncommon_str.append(sentences[l][j - 1])\n",
    "\n",
    "                j -= 1\n",
    "            # if the length of the substring from the previous row is greater, add the uncommon character to uncommon_str2 list and move to the previous row\n",
    "            else:\n",
    "                if tracker_str2 == -1:\n",
    "                    tracker_str2 = i - 1\n",
    "                    sub_uncommon.append(temp_sentence[i - 1])\n",
    "                elif tracker_str2 == i:\n",
    "                    sub_uncommon.append(temp_sentence[i - 1])\n",
    "                    tracker_str2 = i - 1\n",
    "                else:\n",
    "                    sub_uncommon.reverse()\n",
    "                    if l == 1:\n",
    "                        uncommon_str_i[0].append(sub_uncommon if len(sub_uncommon) > 1 else sub_uncommon[0])\n",
    "                    else:\n",
    "                        if '#' not in sub_uncommon:\n",
    "                            sub_uncommon.reverse()\n",
    "                            for k in range(l):\n",
    "                                sub_uncommon_str_temp[k].append(sub_uncommon if len(sub_uncommon) > 1 else sub_uncommon[0])\n",
    "                        else:\n",
    "                            sub_uncommon_copy = sub_uncommon.copy()\n",
    "                            sub_uncommon_str_i_temp.append(sub_uncommon_copy if len(sub_uncommon_copy) > 1 else sub_uncommon_copy[0])\n",
    "                            for k in range(l):\n",
    "                                sub_uncommon_copy = sub_uncommon.copy()\n",
    "                                updated_uncommon_str = uncommon_str_i[k][len(sub_uncommon_str_i_temp) - 1]\n",
    "                                if type(updated_uncommon_str) == list:\n",
    "                                    for term in updated_uncommon_str:\n",
    "                                        if '#' in sub_uncommon_copy:\n",
    "                                            ind = sub_uncommon_copy.index(\"#\")\n",
    "                                            sub_uncommon_copy[ind] = term\n",
    "                                    sub_uncommon_copy = remove_all(sub_uncommon_copy, '#')\n",
    "                                    sub_uncommon_str_temp[k].append(sub_uncommon_copy if len(sub_uncommon) > 1 else sub_uncommon_copy[0])\n",
    "                                else:\n",
    "                                    sub_uncommon_str_temp[k].append(updated_uncommon_str)\n",
    "                    sub_uncommon = []\n",
    "                    tracker_str2 = i - 1\n",
    "                    sub_uncommon.append(temp_sentence[i - 1])\n",
    "\n",
    "                common[index - 1] = \"#\"\n",
    "                index -= 1\n",
    "                i  -= 1\n",
    "\n",
    "        if l == 1:\n",
    "            if len(sub_uncommon) > 0:\n",
    "                sub_uncommon.reverse()\n",
    "                uncommon_str_i[0].append(sub_uncommon if len(sub_uncommon) > 1 else sub_uncommon[0])\n",
    "        else:\n",
    "            for k in range(l):\n",
    "                uncommon_str_i[k] = sub_uncommon_str_temp[k]\n",
    "            if len(sub_uncommon) > 0:\n",
    "                if '#' not in sub_uncommon:\n",
    "                    sub_uncommon.reverse()\n",
    "                    for k in range(l):\n",
    "                        uncommon_str_i[k].append(sub_uncommon if len(sub_uncommon) > 1 else sub_uncommon[0])\n",
    "                else:\n",
    "                    sub_uncommon.reverse()\n",
    "                    for k in range(l):\n",
    "                        sub_uncommon_copy = sub_uncommon.copy()\n",
    "                        for term in uncommon_str_i[k][len(uncommon_str_i[k]) - 1]:\n",
    "                            if '#' in sub_uncommon_copy:\n",
    "                                ind = sub_uncommon_copy.index(\"#\")\n",
    "                                sub_uncommon_copy[ind] = term\n",
    "                        sub_uncommon_copy = remove_all(sub_uncommon_copy, \"#\")\n",
    "                        uncommon_str_i[k][len(uncommon_str_i[k]) - 1] = sub_uncommon_copy\n",
    "\n",
    "        if len(sub_uncommon_str) > 0:\n",
    "            sub_uncommon_str.reverse()\n",
    "            uncommon_str_i[l].append(sub_uncommon_str if len(sub_uncommon_str) > 1 else sub_uncommon_str[0])\n",
    "\n",
    "        temp_sentence = remove_all(common.copy(), \"\")\n",
    "\n",
    "    # join the common list into a sentence\n",
    "    common_sentence = \" \".join(temp_sentence)\n",
    "\n",
    "    # reverse the order of the uncommon substring lists\n",
    "    for i in range(len(uncommon_str_i)):\n",
    "        uncommon_str_i[i].reverse()\n",
    "\n",
    "    # return the common sentence and the lists of uncommon substrings\n",
    "    return shrink(common_sentence), uncommon_str_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 : i love to pay my video games in my free time especially retro video games\n",
      "sentence 2 : i love to play oreo games in my free thyme especially retro video games\n",
      "sentence 3 : i live to slay video vames in my free time especially utro video games\n",
      "\n",
      "Common parts of the sentences : i # to # in my free # especially # video games\n",
      "\n",
      "Uncommon parts of each sentences :\n",
      "\n",
      "sentence 1 : ['love', ['pay', 'my', 'video', 'games'], 'time', 'retro']\n",
      "\n",
      "sentence 2 : ['love', ['play', 'oreo', 'games'], 'thyme', 'retro']\n",
      "\n",
      "sentence 3 : ['live', ['slay', 'video', 'vames'], 'time', 'utro']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    print(\"sentence \" + str(i + 1) +\" : \"+ sentences[i][0])\n",
    "\n",
    "splited_sentences = split_preprocess(sentences)\n",
    "common_sentence, uncommon_str_i = common_and_uncommon_extraction(splited_sentences)\n",
    "print(\"\\nCommon parts of the sentences : \" + common_sentence)\n",
    "\n",
    "print(\"\\nUncommon parts of each sentences :\")\n",
    "for i in range(len(uncommon_str_i)):\n",
    "    print(\"\\nsentence \" + str(i + 1) +\" : \"+str(uncommon_str_i[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) N gram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_lentgh(uncommon_str_i):\n",
    "    lens = []\n",
    "    for i in range(len(uncommon_str_i[0])):\n",
    "        temp = []\n",
    "        for j in range(len(uncommon_str_i)):\n",
    "            temp.append(len(uncommon_str_i[j][i]) if type(uncommon_str_i[j][i]) == list else 1)\n",
    "        lens.append(min(temp))\n",
    "    return lens\n",
    "\n",
    "def get_og_sentence_vector(uncommon_str, common_sentence):\n",
    "    og_sentence_vector = []\n",
    "    temp = common_sentence.split()\n",
    "    i = 0    \n",
    "    for t in temp:\n",
    "        if t == \"#\":\n",
    "            if type(uncommon_str[i]) == list:\n",
    "                og_sentence_vector.extend(uncommon_str[i])\n",
    "            else:\n",
    "                og_sentence_vector.append(uncommon_str[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            og_sentence_vector.append(t)\n",
    "    return og_sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_distribution(uncommon_str_i, common_sentence):\n",
    "    final_uncommon_str_i = []\n",
    "    for i in range(len(uncommon_str_i)):\n",
    "        final_uncommon_str_i.append([])\n",
    "\n",
    "    nb_unc_str = 0\n",
    "\n",
    "    lens = get_gram_lentgh(uncommon_str_i)\n",
    "\n",
    "    for uncommon_str in uncommon_str_i:\n",
    "        for i in range(len(uncommon_str)):\n",
    "            # Make a copy of the current list of the current uncommon part for string 1\n",
    "            unc_str = uncommon_str[i].copy() if type(uncommon_str[i]) == list else [uncommon_str[i]]\n",
    "            while len(unc_str) > lens[i]:\n",
    "                og_sentence = get_og_sentence_vector(uncommon_str, common_sentence)\n",
    "                bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "                # Variable containing the common words that won't allowed in the bigrams\n",
    "                common_words_str = list(set(og_sentence) - set(unc_str))\n",
    "\n",
    "                # Generate a list of all n-grams of size n for the sentence\n",
    "                n_grams_str = list(ngrams(og_sentence, 2))\n",
    "                \n",
    "                # Use the bigram collocation finder to get the best bigrams for the sentence\n",
    "                finder_str = BigramCollocationFinder.from_words(og_sentence)\n",
    "                best_bigrams_str = finder_str.nbest(bigram_measures.pmi, len(n_grams_str))\n",
    "\n",
    "                # Filter out bigrams that contain common words from the current list of uncommon words\n",
    "                best_uncommon_ngrams_str = [ngram for ngram in best_bigrams_str if (not any(p_ngrams in ngram for p_ngrams in common_words_str))]\n",
    "                \n",
    "                # Generate the final list of uncommon n-grams for string 1 by filtering the filtered bigrams and remaining uncommon words\n",
    "                uncommon_ngrams_str = [''] * len(unc_str)\n",
    "                count1 = len(unc_str)\n",
    "                count2 = 0\n",
    "                for b in best_uncommon_ngrams_str:\n",
    "                    if b[0] in unc_str and b[1] in unc_str:\n",
    "                        uncommon_ngrams_str[unc_str.index(b[0])] = \" \".join(list(b))\n",
    "                        count2 += 1\n",
    "                        unc_str[unc_str.index(b[0])] = ''\n",
    "                        unc_str[unc_str.index(b[1])] = ''\n",
    "                        count1 -= 2\n",
    "                    if count1 + count2 == lens[i]:\n",
    "                        break\n",
    "                if unc_str != [\"\"] * len(unc_str):\n",
    "                    for j in range(len(unc_str)):\n",
    "                        if unc_str[j] != '':\n",
    "                            uncommon_ngrams_str[j] = unc_str[j]\n",
    "                uncommon_ngrams_str = remove_all(uncommon_ngrams_str, '')\n",
    "                unc_str = uncommon_ngrams_str.copy()\n",
    "            final_uncommon_str_i[nb_unc_str].append(unc_str)\n",
    "        nb_unc_str += 1\n",
    "    return final_uncommon_str_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['name', 'is james', 'and i', 'work as', 'a data', 'scientists']],\n",
       " [['nail', 'fits', 'jane', 'andy', 'workable', 'scientist']]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_uncommon_str = ngram_distribution(uncommon_str_i, common_sentence)\n",
    "final_uncommon_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Apply the mask on the uncommon parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking(common_sentence, uncommon_str):\n",
    "    for i in range(len(uncommon_str)):\n",
    "        mask = \"[MASK] \" * len(uncommon_str[i])\n",
    "        common_sentence = common_sentence.replace(\"#\", mask, 1)\n",
    "    return \" \".join(common_sentence.split())\n",
    "masked_sentence = masking(common_sentence, final_uncommon_str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 : hello my name is james and i work as a data scientists\n",
      "sentence 2 : hello my nail fits jane andy workable scientist\n",
      "\n",
      "Common parts of the 2 sentences with the uncommon parts masked :\n",
      "hello my [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "\n",
      "Uncommon parts of each sentence : \n",
      "\n",
      "sentence 1 : [['name', 'is james', 'and i', 'work as', 'a data', 'scientists']]\n",
      "\n",
      "sentence 2 : [['nail', 'fits', 'jane', 'andy', 'workable', 'scientist']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    print(\"sentence \" + str(i + 1) +\" : \"+ sentences[i][0])\n",
    "print(\"\\nCommon parts of the 2 sentences with the uncommon parts masked :\\n\"+ masked_sentence) \n",
    "print(\"\\nUncommon parts of each sentence : \")\n",
    "for i in range(len(final_uncommon_str)):\n",
    "    print(\"\\nsentence \" + str(i + 1) +\" : \"+str(final_uncommon_str[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

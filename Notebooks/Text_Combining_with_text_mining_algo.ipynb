{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>First method</h1></center>\n",
    "<center><h1>Text combining using Text mining, MLM and Word embeddings</h1></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Make the text mining algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence[0]\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return [sentence]\n",
    "\n",
    "# Transform the sentences into vectors\n",
    "def split_preprocess(sentences):\n",
    "    s = []\n",
    "    for sentence in sentences:\n",
    "        s.append(sentence[0].split())\n",
    "    return s\n",
    "\n",
    "# Reduce the sequences of # into one #\n",
    "def shrink(sentence):\n",
    "    temp = sentence.split()\n",
    "    b = False\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] == \"#\" and b:\n",
    "            temp[i] = \"\"\n",
    "        elif temp[i] == \"#\" and not b:\n",
    "            b = True\n",
    "        elif temp[i] != \"#\" and b:\n",
    "            b = False\n",
    "    while \"\" in temp:       \n",
    "        temp.remove(\"\")\n",
    "    \n",
    "    return \" \".join(temp)\n",
    "\n",
    "def flatten(final_uncommon_str):\n",
    "    flatten_final_uncommon_str = []\n",
    "    for i in range(len(final_uncommon_str)):\n",
    "        flatten_final_uncommon_str.append([item for sublist in final_uncommon_str[i] for item in sublist])\n",
    "    return flatten_final_uncommon_str\n",
    "\n",
    "# Init the Dynamic matrix\n",
    "def init_matrix(temp_sentence, sentences, lenght, l):\n",
    "        # initialize the L matrix with zeros\n",
    "        L = [[0] * (lenght + 1) for _ in range(len(temp_sentence) + 1)]\n",
    "\n",
    "        # fill in the L matrix using dynamic programming\n",
    "        for i in range(len(temp_sentence) + 1):\n",
    "            for j in range(lenght + 1):\n",
    "                # if either string is empty, the longest common substring is zero\n",
    "                if i == 0 or j == 0:\n",
    "                    L[i][j] = 0\n",
    "                # if the characters match, add one to the length of the longest common substring\n",
    "                elif temp_sentence[i - 1] == sentences[l][j - 1]:\n",
    "                    L[i][j] = L[i - 1][j - 1] + 1\n",
    "                # if the characters don't match, take the maximum length from the previous row or column\n",
    "                else:\n",
    "                    L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "        return L\n",
    "\n",
    "# init list of lists\n",
    "def init_list_of_lists(lenght):\n",
    "    list_of_lists = []\n",
    "    for i in range(lenght):\n",
    "        list_of_lists.append([])\n",
    "    return list_of_lists\n",
    "\n",
    "# remove all the occourences of a value in a list\n",
    "def remove_all(liste, value):\n",
    "    while value in liste:\n",
    "        liste.remove(value)\n",
    "    return liste\n",
    "\n",
    "# get the length of the smallest n gram\n",
    "def get_gram_lentgh(uncommon_str_i):\n",
    "    lens = []\n",
    "    for i in range(len(uncommon_str_i[0])):\n",
    "        temp = []\n",
    "        for j in range(len(uncommon_str_i)):\n",
    "            temp.append(len(uncommon_str_i[j][i]) if type(uncommon_str_i[j][i]) == list else 1)\n",
    "        lens.append(min(temp))\n",
    "    return lens\n",
    "\n",
    "# get the original sentence in a vector form\n",
    "def get_og_sentence_vector(uncommon_str, common_sentence):\n",
    "    og_sentence_vector = []\n",
    "    temp = common_sentence.split()\n",
    "    i = 0    \n",
    "    for t in temp:\n",
    "        if t == \"#\":\n",
    "            if type(uncommon_str[i]) == list:\n",
    "                og_sentence_vector.extend(uncommon_str[i])\n",
    "            else:\n",
    "                og_sentence_vector.append(uncommon_str[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            og_sentence_vector.append(t)\n",
    "    return og_sentence_vector\n",
    "\n",
    "def masking(common_sentence, uncommon_str):\n",
    "    for i in range(len(uncommon_str)):\n",
    "        mask = \"[MASK] \" * len(uncommon_str[i])\n",
    "        common_sentence = common_sentence.replace(\"#\", mask, 1)\n",
    "    return \" \".join(common_sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_and_uncommon_extraction(sentences):\n",
    "    lens = [len(s) for s in sentences]\n",
    "\n",
    "    # initialize the uncommon substring lists\n",
    "    uncommon_str_i = init_list_of_lists(len(sentences))\n",
    "\n",
    "    temp_sentence = sentences[0]\n",
    "    for l in range(1, len(sentences)):\n",
    "        # initialize the L matrix\n",
    "        L = init_matrix(temp_sentence, sentences, lens[l], l)\n",
    "\n",
    "        # calculate the index based on the length of the longer string\n",
    "        index = max(len(temp_sentence), lens[l])\n",
    "\n",
    "        # initialize the common list with empty strings\n",
    "        common = [\"\"] * (index + 1)\n",
    "        common[index] = \"\"\n",
    "\n",
    "        # set i and j to the end of each string\n",
    "        i = len(temp_sentence)\n",
    "        j = lens[l]\n",
    "\n",
    "        # trackers to follow the uncommon substrings position\n",
    "        tracker_str1 = -1 \n",
    "        tracker_str2 = -1\n",
    "        # lists that save a sequence of uncommon substrings\n",
    "        sub_uncommon_str = []\n",
    "        sub_uncommon = []\n",
    "        # final list that contains all the uncommon substrings\n",
    "        sub_uncommon_str_i_temp = []\n",
    "        sub_uncommon_str_temp = init_list_of_lists(len(sentences))\n",
    "\n",
    "        # loop through the L matrix to find the common and uncommon substrings\n",
    "        while i > 0 and j > 0:\n",
    "            # if the characters match, add the character to the common list and move to the previous diagonal cell\n",
    "            if temp_sentence[i - 1] == sentences[l][j - 1]:\n",
    "                common[index - 1] = temp_sentence[i - 1]\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "                index -= 1\n",
    "            # if the length of the substring from the previous column is greater, add the uncommon character to uncommon_str list and move to the previous column\n",
    "            elif L[i - 1][j] < L[i][j - 1]:\n",
    "                if tracker_str1 == -1: # if the tracker is -1, it means that the substring is the first one\n",
    "                    tracker_str1 = j - 1\n",
    "                    sub_uncommon_str.append(sentences[l][j - 1]) # add the uncommon character to the list\n",
    "                elif tracker_str1 == j: # if the tracker is equal to the current index, it means that the substring is part of the same sequence\n",
    "                    sub_uncommon_str.append(sentences[l][j - 1]) # add the uncommon character to the sequence list\n",
    "                    tracker_str1 = j - 1\n",
    "                else: # if the tracker is not equal to the current index, it means that the substring is part of a different sequence\n",
    "                    sub_uncommon_str.reverse() \n",
    "                    # add the sequence to the final list\n",
    "                    uncommon_str_i[l].append(sub_uncommon_str if len(sub_uncommon_str) > 1 else sub_uncommon_str[0])\n",
    "                    sub_uncommon_str = [] # reset the sequence list\n",
    "                    tracker_str1 = j - 1 # reset the tracker to the first uncommon string of the new sequence\n",
    "                    sub_uncommon_str.append(sentences[l][j - 1]) # add the uncommon string to the new sequence list\n",
    "\n",
    "                j -= 1 # move to the previous column\n",
    "            # if the length of the substring from the previous row is greater, add the uncommon character to uncommon_str2 list and move to the previous row\n",
    "            else:\n",
    "                if tracker_str2 == -1: # if the tracker is -1, it means that the substring is the first one\n",
    "                    tracker_str2 = i - 1\n",
    "                    sub_uncommon.append(temp_sentence[i - 1]) # add the uncommon character to the list\n",
    "                elif tracker_str2 == i: # if the tracker is equal to the current index, it means that the substring is part of the same sequence\n",
    "                    sub_uncommon.append(temp_sentence[i - 1]) # add the uncommon character to the sequence list\n",
    "                    tracker_str2 = i - 1\n",
    "                else: # if the tracker is not equal to the current index, it means that the substring is part of a different sequence\n",
    "                    sub_uncommon.reverse()\n",
    "                    if l == 1: # if the index point to the second string, it means we are dealing with the first string so we add the sequence to the final list \n",
    "                        uncommon_str_i[0].append(sub_uncommon if len(sub_uncommon) > 1 else sub_uncommon[0])\n",
    "                    else: # else it means that we are dealing with the common sentence \n",
    "                        \n",
    "                        if '#' not in sub_uncommon: # if the sequence doesn't contain the # character, it means it is a new sequence so we add it to the final list directly\n",
    "                            sub_uncommon.reverse()\n",
    "                            # we add the uncommon substring to all the uncommon parts of all the previous strings\n",
    "                            for k in range(l):\n",
    "                                sub_uncommon_str_temp[k].append(sub_uncommon if len(sub_uncommon) > 1 else sub_uncommon[0])\n",
    "                        else: # if the sequence contains the # character, it means that it is a sequence that is part of a previous sequence so we need to update it\n",
    "                            sub_uncommon_copy = sub_uncommon.copy()\n",
    "                            # we add the uncommon substring to a temp list to not mess up the order of the final list\n",
    "                            sub_uncommon_str_i_temp.append(sub_uncommon_copy if len(sub_uncommon_copy) > 1 else sub_uncommon_copy[0])\n",
    "                            for k in range(l):\n",
    "                                sub_uncommon_copy = sub_uncommon.copy()\n",
    "                                # we get the last uncommon substring of the previous string\n",
    "                                updated_uncommon_str = uncommon_str_i[k][len(sub_uncommon_str_i_temp) - 1]\n",
    "                                if type(updated_uncommon_str) == list: # if the last uncommon substring is a list, it means that it is a sequence so we need to update it\n",
    "                                    for term in updated_uncommon_str: # we loop through the sequence and replace the # character with the uncommon substring\n",
    "                                        if '#' in sub_uncommon_copy:\n",
    "                                            ind = sub_uncommon_copy.index(\"#\")\n",
    "                                            sub_uncommon_copy[ind] = term\n",
    "                                    sub_uncommon_copy = remove_all(sub_uncommon_copy, '#') # we remove all the # characters that are left\n",
    "                                    sub_uncommon_str_temp[k].append(sub_uncommon_copy if len(sub_uncommon) > 1 else sub_uncommon_copy[0]) # we add the updated uncommon substring to the final list\n",
    "                                else:\n",
    "                                    sub_uncommon_str_temp[k].append(updated_uncommon_str) # if the last uncommon substring is not a list, it means that it is a sequence of a single string so we add it to the final list\n",
    "                    sub_uncommon = [] # reset the sequence list\n",
    "                    tracker_str2 = i - 1 # reset the tracker to the first uncommon string of the new sequence\n",
    "                    sub_uncommon.append(temp_sentence[i - 1]) # add the uncommon string to the new sequence list\n",
    "\n",
    "                common[index - 1] = \"#\" # add the # character to the common substring to indicate that an uncommon substring is there\n",
    "                index -= 1 # move to the previous row\n",
    "                i  -= 1 # move to the next string\n",
    "\n",
    "        if l == 1: # if the index point to the second string, it means we are dealing with the first string \n",
    "            if len(sub_uncommon) > 0: # if the length of the substring is greater than 0, it means that there is an uncommon substring left\n",
    "                sub_uncommon.reverse()\n",
    "                uncommon_str_i[0].append(sub_uncommon if len(sub_uncommon) > 1 else sub_uncommon[0]) # add the uncommon substring to the final list\n",
    "        else: # else it means that we are dealing with the common sentence\n",
    "            if len(sub_uncommon) > 0: # if the length of the substring is greater than 0, it means that there is an uncommon substring left\n",
    "                if '#' not in sub_uncommon: # if the sequence doesn't contain the # character, it means it is a new sequence so we add it to the final list directly\n",
    "                    sub_uncommon.reverse()\n",
    "                    for k in range(l):\n",
    "                        uncommon_str_i[k].append(sub_uncommon if len(sub_uncommon) > 1 else sub_uncommon[0])\n",
    "                else: # if the sequence contains the # character, it means that it is a sequence that is part of a previous sequence so we need to update it\n",
    "                    sub_uncommon.reverse()\n",
    "                    \n",
    "                    for k in range(l):\n",
    "                        sub_uncommon_copy = sub_uncommon.copy()\n",
    "                        if len(sub_uncommon_copy) < 2: # if the length of the uncommon substring is less than 2, it means that it is a sequence of a single string so we just replace the # character with the uncommon substring\n",
    "                            sub_uncommon_copy = uncommon_str_i[k][len(uncommon_str_i[k]) - 1][0] if type(uncommon_str_i[k][len(uncommon_str_i[k]) - 1]) == list else uncommon_str_i[k][len(uncommon_str_i[k]) - 1]\n",
    "                        else: # if the length of the uncommon substring is greater than 2, it means that it is a sequence so we need to update it\n",
    "                            if type(uncommon_str_i[k][len(uncommon_str_i[k]) - 1]) == list :\n",
    "                                # we loop through the terms of the sequence that needs to be updated and replace the # character with the uncommon substring\n",
    "                                for term in uncommon_str_i[k][len(uncommon_str_i[k]) - 1]:\n",
    "                                    if '#' in sub_uncommon_copy:\n",
    "                                        ind = sub_uncommon_copy.index(\"#\")\n",
    "                                    sub_uncommon_copy[ind] = term\n",
    "                            else: # if the last uncommon substring is not a list, it means that it is a sequence of a single string so we just replace the # character with the uncommon substring\n",
    "                                ind = sub_uncommon_copy.index(\"#\")\n",
    "                                sub_uncommon_copy[ind] = uncommon_str_i[k][len(uncommon_str_i[k]) - 1]\n",
    "                        sub_uncommon_copy = remove_all(sub_uncommon_copy, \"#\") # we remove all the # characters that are left\n",
    "                        sub_uncommon_str_temp[k].append(sub_uncommon_copy) # we add the updated uncommon substring to the final list\n",
    "            # we add the uncommon substring to all the uncommon parts of all the previous strings\n",
    "            for k in range(l):\n",
    "                uncommon_str_i[k] = sub_uncommon_str_temp[k]\n",
    "\n",
    "        # we add the uncommon substring left to the current string\n",
    "        if len(sub_uncommon_str) > 0:\n",
    "            sub_uncommon_str.reverse()\n",
    "            uncommon_str_i[l].append(sub_uncommon_str if len(sub_uncommon_str) > 1 else sub_uncommon_str[0])\n",
    "        \n",
    "        if j != 0:\n",
    "            sub_uncommon_str = [] # reset the sequence list\n",
    "            while j > 0:\n",
    "                sub_uncommon_str.append(sentences[l][j - 1])\n",
    "                j -= 1\n",
    "            sub_uncommon_str.reverse()\n",
    "            # add the sequence to the final list\n",
    "            uncommon_str_i[l].append(sub_uncommon_str if len(sub_uncommon_str) > 1 else sub_uncommon_str[0])\n",
    "\n",
    "        temp_sentence = remove_all(common.copy(), \"\") # we update the common sentence\n",
    "\n",
    "    # join the common list into a sentence\n",
    "    common_sentence = \" \".join(temp_sentence)\n",
    "\n",
    "    # reverse the order of the uncommon substring lists\n",
    "    for i in range(len(uncommon_str_i)):\n",
    "        uncommon_str_i[i].reverse()\n",
    "\n",
    "    # return the common sentence and the lists of uncommon substrings\n",
    "    return shrink(common_sentence), uncommon_str_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_distribution(uncommon_str_i, common_sentence):\n",
    "    # Initialize the list of lists that will contain the n-grams\n",
    "    final_uncommon_str_i = init_list_of_lists(len(uncommon_str_i))\n",
    "\n",
    "    nb_unc_str = 0\n",
    "\n",
    "    lens = get_gram_lentgh(uncommon_str_i) # get the length of the smallest n grams\n",
    "\n",
    "    for uncommon_str in uncommon_str_i:\n",
    "        for i in range(len(uncommon_str)):\n",
    "            # Make a copy of the current list of the current uncommon part for string 1\n",
    "            unc_str = uncommon_str[i].copy() if type(uncommon_str[i]) == list else [uncommon_str[i]]\n",
    "            while len(unc_str) > lens[i]:\n",
    "                og_sentence = get_og_sentence_vector(uncommon_str, common_sentence)\n",
    "                bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "                # Variable containing the common words that won't allowed in the bigrams\n",
    "                common_words_str = list(set(og_sentence) - set(unc_str))\n",
    "\n",
    "                # Generate a list of all n-grams of size n for the sentence\n",
    "                n_grams_str = list(ngrams(og_sentence, 2))\n",
    "                \n",
    "                # Use the bigram collocation finder to get the best bigrams for the sentence\n",
    "                finder_str = BigramCollocationFinder.from_words(og_sentence)\n",
    "                best_bigrams_str = finder_str.nbest(bigram_measures.pmi, len(n_grams_str))\n",
    "\n",
    "                # Filter out bigrams that contain common words from the current list of uncommon words\n",
    "                best_uncommon_ngrams_str = [ngram for ngram in best_bigrams_str if (not any(p_ngrams in ngram for p_ngrams in common_words_str))]\n",
    "                \n",
    "                # Generate the final list of uncommon n-grams for string 1 by filtering the filtered bigrams and remaining uncommon words\n",
    "                uncommon_ngrams_str = [''] * len(unc_str)\n",
    "                count1 = len(unc_str)\n",
    "                count2 = 0\n",
    "                # We loop through the best uncommon n-grams and check if they are in the uncommon words list\n",
    "                for b in best_uncommon_ngrams_str:\n",
    "                    if b[0] in unc_str and b[1] in unc_str: # if both words are in the uncommon words list\n",
    "                        uncommon_ngrams_str[unc_str.index(b[0])] = \" \".join(list(b)) # we add the n-gram to the final list\n",
    "                        count2 += 1 # we increment the number of uncommon n-grams in the final list\n",
    "                        # we remove the words of the bi-gram from the uncommon words list\n",
    "                        unc_str[unc_str.index(b[0])] = '' \n",
    "                        unc_str[unc_str.index(b[1])] = ''\n",
    "                        count1 -= 2 # we decrement the number of uncommon words in the uncommon words list\n",
    "                    if count1 + count2 == lens[i]: # if we have the number of uncommon n-grams we want\n",
    "                        break\n",
    "                if unc_str != [\"\"] * len(unc_str): # if there are still uncommon words left\n",
    "                    for j in range(len(unc_str)):\n",
    "                        if unc_str[j] != '':\n",
    "                            uncommon_ngrams_str[j] = unc_str[j] # we add the uncommon words left to the final list\n",
    "                uncommon_ngrams_str = remove_all(uncommon_ngrams_str, '') # we remove the empty strings from the final list\n",
    "                unc_str = uncommon_ngrams_str.copy() # we update the current list of uncommon words\n",
    "            \n",
    "            final_uncommon_str_i[nb_unc_str].append(unc_str) # we add the final list of uncommon n-grams to the final list of lists\n",
    "        nb_unc_str += 1 # we increment the number of uncommon parts\n",
    "    return final_uncommon_str_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_mining_algo(sentences):\n",
    "    # Preprocess the sentences\n",
    "    sentences = np.apply_along_axis(preprocess, 1, sentences)\n",
    "    splited_sentences = split_preprocess(sentences)\n",
    "\n",
    "    # Common and Uncommon parts extraction\n",
    "    common_sentence, uncommon_str_i = common_and_uncommon_extraction(splited_sentences)\n",
    "\n",
    "    # N-gram distribution\n",
    "    final_uncommon_str = ngram_distribution(uncommon_str_i, common_sentence)\n",
    "\n",
    "    # Masking\n",
    "    masked_sentence = masking(common_sentence, final_uncommon_str[0])\n",
    "\n",
    "    return masked_sentence, flatten(final_uncommon_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Make the text combining function using Text mining, MLM with BERT and Word Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_combining(sentences):\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        print(\"sentence \" + str(i + 1) +\" : \"+ sentences[i][0])\n",
    "\n",
    "    # Text mining algorithm\n",
    "    masked_sentence, final_uncommon_str = text_mining_algo(sentences)\n",
    "    \n",
    "    # MLM with BERT\n",
    "    fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "    pred = fill_mask(masked_sentence)\n",
    "\n",
    "    # Similarity between the masked words and the uncommon words with word embeddings\n",
    "    nlp = spacy.load(\"en_core_web_md\")  \n",
    "    selected_words = []\n",
    "    for i in range(len(pred)):\n",
    "        df1 = pd.DataFrame(pred[i]) # Convert the prediction to a dataframe\n",
    "        word_list = df1[\"token_str\"].tolist() # Get the list of words from the dataframe\n",
    "        # Get the list of uncommon words for the current masked word\n",
    "        strings = []\n",
    "        for fus in final_uncommon_str:\n",
    "            strings.append(fus[i])\n",
    "\n",
    "        # Get the similarity between the masked word and the uncommon words\n",
    "        similarity = []\n",
    "        for s in strings:\n",
    "            similarity.append(np.mean([nlp(w).similarity(nlp(s)) for w in word_list]))\n",
    "\n",
    "        # Select the uncommon word with the highest similarity\n",
    "        selected_words.append(strings[np.argmax(similarity)])\n",
    "\n",
    "    # Combine the masked sentence with the selected words\n",
    "    combined_sentence = masked_sentence\n",
    "    for index in range(len(selected_words)):\n",
    "        combined_sentence = combined_sentence.replace(\"[MASK]\", selected_words[index], 1)\n",
    "\n",
    "    print(\"Combined Sentence: \", combined_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Test the text combining algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 : I love to pay my video games in my free rime, especially retro video games.\n",
      "sentence 2 : Y luve to play oreo games in my free thyme, especially retro video games.\n",
      "sentence 3 : Ay live to slay video vames in my free time, especially utro video games.\n",
      "Combined Sentence:  i love to play video games in my free time especially retro video games\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"I love to pay my video games in my free rime, especially retro video games.\"\n",
    "sentence2 = \"Y luve to play oreo games in my free thyme, especially retro video games.\"\n",
    "sentence3 = \"Ay live to slay video vames in my free time, especially utro video games.\"\n",
    "sentences = np.array([[sentence1], [sentence2], [sentence3]])\n",
    "\n",
    "text_combining(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

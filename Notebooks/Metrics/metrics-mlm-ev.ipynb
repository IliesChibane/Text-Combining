{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Evaluation of the MLM models</h1></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_bert = pd.read_excel(\"Datasets.dvc/bert-base-uncased.xlsx\")\n",
    "df_roberta = pd.read_excel(\"Datasets.dvc/roberta-base_pred.xlsx\")\n",
    "df_distilroberta = pd.read_excel(\"Datasets.dvc/distilroberta-base_pred.xlsx\")\n",
    "df_albert = pd.read_excel(\"Datasets.dvc/albert-base_pred.xlsx\")\n",
    "df_distilbert = pd.read_excel(\"Datasets.dvc/distilbert-base-uncased_pred_ngram.xlsx\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "def compute_pll(sentences, model_name= 'bert-base-uncased'):\n",
    "    # Gett model n tokenizer \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "    # Init  PLL values\n",
    "    pll_values = []\n",
    "\n",
    "    # Cal PLL for chaque sentence bu\n",
    "    for sentence in sentences:\n",
    "        # Tokenizeing\n",
    "        input_ids = tokenizer.encode(sentence, return_tensors='tf')\n",
    "        input_ids = input_ids[0]\n",
    "\n",
    "        # Init pll value \n",
    "        pll = 0\n",
    "\n",
    "        # Cal cond log probability for chaue jeton \n",
    "        for i in range(1, len(input_ids)-1):\n",
    "            # Create input with masked token\n",
    "            masked_input_ids = input_ids.numpy().copy()\n",
    "            masked_input_ids[i] = tokenizer.mask_token_id\n",
    "\n",
    "            # Cal log probabilities for masked eton\n",
    "            outputs = model(tf.convert_to_tensor(masked_input_ids[None, :]))\n",
    "            log_probs = tf.nn.log_softmax(outputs[0][0, i], axis=0)\n",
    "\n",
    "            # Update pll\n",
    "            pll += log_probs[input_ids[i]].numpy()\n",
    "\n",
    "        # Append PLL value to list\n",
    "        pll_values.append(abs(pll))\n",
    "\n",
    "    return pll_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This is love\n",
      "PLL: 13.75\n",
      "\n",
      "Sentence: This is awsome\n",
      "PLL: 38.41\n",
      "\n",
      "Sentence: I love you\n",
      "PLL: 7.48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = ['This is love', 'This is awsome', 'I love you']\n",
    "model_name = 'bert-base-uncased'\n",
    "pll_values = compute_pll(sentences, model_name)\n",
    "\n",
    "for sentence, pll in zip(sentences, pll_values):\n",
    "    print(f'Sentence: {sentence}')\n",
    "    print(f'PLL: {pll:.2f}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForMaskedLM, TFAutoModelForCausalLM\n",
    "\n",
    "def calculate_perplexities(sentences, model_name='bert-base-uncased'):\n",
    "    if model_name in ['distilbert-base-uncased', 'albert-base-v2']:\n",
    "        model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    else:\n",
    "        model = TFAutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    perplexities = []\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer.encode(sentence, return_tensors='tf')\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        perplexity = tf.math.exp(loss).numpy()[0]\n",
    "        perplexities.append(perplexity)\n",
    "    return perplexities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.645866, 274.9464]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "sentences = ['Life is life.', 'I love natural language processing.']\n",
    "\n",
    "perplexities = calculate_perplexities(sentences, model_name)\n",
    "\n",
    "print(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def shortest_path_distance(s1, s2):\n",
    "    synset1 = wn.synset(s1)\n",
    "    synset2 = wn.synset(s2)\n",
    "    distance = synset1.shortest_path_distance(synset2)\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_similarity(s1, s2):\n",
    "    distance = shortest_path_distance(s1, s2)\n",
    "    if distance is None:\n",
    "        return 0\n",
    "    similarity = 1 / (distance + 1)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracies(model_name, sentences):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    accuracies = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenizeing\n",
    "        input_ids = tokenizer.encode(sentence, return_tensors='tf')\n",
    "        # Masking\n",
    "        mask_index = 2\n",
    "        original_word = sentence.split()[mask_index]\n",
    "        input_ids_np = input_ids.numpy()\n",
    "        input_ids_np[0][mask_index] = tokenizer.mask_token_id\n",
    "        input_ids = tf.convert_to_tensor(input_ids_np)\n",
    "        # predictions\n",
    "        outputs = model(input_ids)\n",
    "        predictions = tf.argsort(outputs[0][0][mask_index], direction='DESCENDING')\n",
    "        similarities = []\n",
    "        for prediction in predictions:\n",
    "            predicted_word = tokenizer.decode([prediction])\n",
    "            synsets1 = wn.synsets(original_word)\n",
    "            synsets2 = wn.synsets(predicted_word)\n",
    "            if synsets1 and synsets2:\n",
    "                max_similarity = max(path_similarity(synset1.name(), synset2.name()) for synset1 in synsets1 for synset2 in synsets2)\n",
    "                similarities.append(max_similarity)\n",
    "            else:\n",
    "                similarities.append(0)\n",
    "        accuracy = max(similarities)\n",
    "        accuracies.append(accuracy)\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.5]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "sentences = ['I love me', 'I love oreo games']\n",
    "accuracies = calculate_accuracies(model_name, sentences)\n",
    "print(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import TFAutoModelForMaskedLM, AutoTokenizer ######fillmask\n",
    "\n",
    "def calculate_execution_time(sentences, n_runs=10, model_name='bert-base-uncased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    execution_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start_time = time.time()\n",
    "        for sentence in sentences:\n",
    "            \n",
    "            ## we actually evaluate fillna not token stuff\n",
    "            input_ids = tokenizer.encode(sentence, return_tensors='tf')\n",
    "            model(input_ids)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        execution_times.append(execution_time)\n",
    "    min_time = min(execution_times)\n",
    "    max_time = max(execution_times)\n",
    "    normalized_times = [(x - min_time) / (max_time - min_time) for x in execution_times]\n",
    "    return normalized_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "sentences = ['The cat sat on the mat', 'What does the fox say ?']\n",
    "normalized_times = calculate_execution_time(sentences, 10,model_name)\n",
    "print(normalized_times)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('dataset.xlsx')\n",
    "df = df.iloc[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"bert-base-uncased\", \"roberta-base\", \"albert-base-v2\", \"distilroberta-base\", \"distilbert-base-uncased\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def calculate_metrics(df, models):\n",
    "    metrics = {}\n",
    "    for model in models:\n",
    "        pll_values = [x for x in compute_pll(df, model)]\n",
    "        perplexities = [x for x in calculate_perplexities(df, model)]\n",
    "        accuracies = calculate_accuracies(model, df)\n",
    "        execution_times = calculate_execution_time(df, model_name=model)\n",
    "\n",
    "        min_pll, max_pll = min(pll_values), max(pll_values)\n",
    "        min_perplexity, max_perplexity = min(perplexities), max(perplexities)\n",
    "        min_accuracy, max_accuracy = min(accuracies), max(accuracies)\n",
    "        min_execution_time, max_execution_time = min(execution_times), max(execution_times)\n",
    "\n",
    "        metrics[model] = {\n",
    "            'average_pll': 1 - ((mean(pll_values) - min_pll) / (max_pll - min_pll)),\n",
    "            'average_perplexity': 1 - ((mean(perplexities) - min_perplexity) / (max_perplexity - min_perplexity)),\n",
    "            'average_accuracy': (mean(accuracies) - min_accuracy) / (max_accuracy - min_accuracy),\n",
    "            'average_execution_time': 1 - ((mean(execution_times) - min_execution_time) / (max_execution_time - min_execution_time))\n",
    "        }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_metrics(df[:500], models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics):\n",
    "    n_metrics = len(next(iter(metrics.values())))\n",
    "    n_models = len(metrics)\n",
    "    x = np.arange(n_models) * 2\n",
    "    width = 0.8 / n_metrics\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for i, (metric_name, metric_values) in enumerate(zip(next(iter(metrics.values())).keys(), zip(*[model_metrics.values() for model_metrics in metrics.values()]))):\n",
    "        bar_positions = x + i * width - 0.4\n",
    "        ax.bar(bar_positions, metric_values, width, label=metric_name)\n",
    "        for j, value in enumerate(metric_values):\n",
    "            ax.text(bar_positions[j], value, f'{value:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics.keys())\n",
    "    ax.legend()\n",
    "    plt.savefig('metrique MLM-100-correct.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = {\n",
    "    'bert-base-uncased': 'BERT',\n",
    "    'roberta-base': 'RoBERTa',\n",
    "    'albert-base-v2': 'AlBERT',\n",
    "    'distilroberta-base': 'DistilRoBERTa',\n",
    "    'distilbert-base-uncased': 'DistilBERT'\n",
    "}\n",
    "\n",
    "normalized_metrics = {}\n",
    "for model, model_metrics in metrics.items():\n",
    "    normalized_model_metrics = {}\n",
    "    for metric_name, metric_value in model_metrics.items():\n",
    "        normalized_model_metrics[metric_name] = metric_value\n",
    "    normalized_metrics[model_names[model]] = normalized_model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(normalized_metrics)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = [0.44, 0.5, 0.09, 0.39, 0.39]\n",
    "perplexity = [0.21, 0.19, 0.66, 0.3, 0.32]\n",
    "execution_time = [0.56, 0.6, 0.48, 0.38, 0.35]\n",
    "\n",
    "bins = [\"BERT\", \"RoBERTa\", \"ALBERT\", \"DistilRoBERTa\", \"DistilBERT\"]\n",
    "\n",
    "# Visualize the accuracy, perplexity and execution time of each model in a bar chart all in the same plot\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "ax[0].bar(bins, accuracy)\n",
    "ax[0].set_title(\"Accuracy\")\n",
    "ax[1].bar(bins, perplexity)\n",
    "ax[1].set_title(\"Perplexity\")\n",
    "ax[2].bar(bins, execution_time)\n",
    "ax[2].set_title(\"Execution time (s)\")\n",
    "\n",
    "# add title\n",
    "fig.suptitle(\"Performance of the models on a test dataset\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def remove_all(liste, value):\n",
    "    while value in liste:\n",
    "        liste.remove(value)\n",
    "    return liste\n",
    "\n",
    "def similarity(word, list_of_words):\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Get the synsets of the word\n",
    "    synsets_word = wn.synsets(lemmatizer.lemmatize(word))\n",
    "\n",
    "    # Calculate the similarity between the word and each word in the list\n",
    "    similarities = []\n",
    "    for w in list_of_words:\n",
    "        synsets_w = wn.synsets(lemmatizer.lemmatize(w))\n",
    "        max_sim = 0\n",
    "        for synset_word in synsets_word:\n",
    "            for synset_w in synsets_w:\n",
    "                sim = synset_word.path_similarity(synset_w)\n",
    "                if sim is not None and sim > max_sim:\n",
    "                    max_sim = sim\n",
    "        similarities.append(max_sim)\n",
    "\n",
    "    if word in list_of_words:\n",
    "        similarities.append(1)\n",
    "\n",
    "    # Print the list of similarities\n",
    "    return np.max(similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Test metrics</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import numpy as np\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "def calculate_metrics(reference, hypothesis):\n",
    "    # Calculate BLEU score\n",
    "    bleu = sentence_bleu([reference], hypothesis)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(' '.join(hypothesis), ' '.join(reference))\n",
    "    rouge_1 = rouge_scores[0]['rouge-1']['f']\n",
    "    rouge_2 = rouge_scores[0]['rouge-2']['f']\n",
    "    rouge_l = rouge_scores[0]['rouge-l']['f']\n",
    "    \n",
    "    # Calculate METEOR score\n",
    "    meteor = meteor_score([reference], hypothesis)\n",
    "    \n",
    "    # Calculate Word Error Rate (WER)\n",
    "    wer = edit_distance(reference, hypothesis) / len(reference)\n",
    "    \n",
    "    return {'bleu': bleu, 'rouge-1': rouge_1, 'rouge-2': rouge_2, 'rouge-l': rouge_l, 'meteor': meteor, 'wer': wer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 6.2705618118895185e-78, 'rouge-1': 0.7499999953125, 'rouge-2': 0.6666666622222223, 'rouge-l': 0.7499999953125, 'meteor': 0.6134259259259259, 'wer': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rania\\anaconda3\\envs\\mybase\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "reference = ['the', 'cat', 'is', 'on', 'the', 'mat']\n",
    "hypothesis = ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']\n",
    "reference = 'I love you so much'.split()\n",
    "hypothesis = 'I love you'.split()\n",
    "scores = calculate_metrics(reference, hypothesis)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score with lower maximum n-gram order: 0.48795003647426655\n",
      "BLEU score with smoothing function: 0.18575057999133598\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "reference = ['the', 'cat', 'is', 'on', 'the', 'mat']\n",
    "hypothesis = ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']\n",
    "\n",
    "# Use a lower maximum n-gram order\n",
    "bleu = sentence_bleu([reference], hypothesis, weights=(0.5, 0.5))\n",
    "print(f'BLEU score with lower maximum n-gram order: {bleu}')\n",
    "\n",
    "# Use a smoothing function\n",
    "smooth = SmoothingFunction().method1\n",
    "bleu = sentence_bleu([reference], hypothesis, smoothing_function=smooth)\n",
    "print(f'BLEU score with smoothing function: {bleu}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links to know more about the metrics and why i chose them :\n",
    "- [Bleu score](https://towardsdatascience.com/-foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b#:~:text=Bleu%20Scores%20are%20between%200,rarely%20achieve%20a%20perfect%20match.)\n",
    "- [Rouge score](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499)\n",
    "- [METEOR_SCORE-SECTION3](https://medium.com/explorations-in-language-and-learning/metrics-for-nlg-evaluation-c89b6a781054)\n",
    "- WER : word error rate is just a very basic formula to see the error rate in a sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from random import random\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "class VocabularyProcessor:\n",
    "    def __init__(self, max_document_length=None, min_frequency=0):\n",
    "        self.max_document_length = max_document_length\n",
    "        self.min_frequency = min_frequency\n",
    "        self.vocab_dict = {}\n",
    "        self.vocab_list = []\n",
    "        self.reverse_vocab_dict = {}\n",
    "        self.stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, documents):\n",
    "        # Convert all text to lowercase\n",
    "        documents = [doc.lower() for doc in documents]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        documents = [nltk.word_tokenize(doc) for doc in documents]\n",
    "        \n",
    "        # Remove stop words\n",
    "        documents = [[word for word in doc if word not in self.stop_words] for doc in documents]\n",
    "        \n",
    "        # Lemmatize the text\n",
    "        documents = [[self.lemmatizer.lemmatize(word) for word in doc] for doc in documents]\n",
    "        \n",
    "        # Remove rare words\n",
    "        if self.min_frequency > 0:\n",
    "            word_counts = {}\n",
    "            for doc in documents:\n",
    "                for word in doc:\n",
    "                    if word in word_counts:\n",
    "                        word_counts[word] += 1\n",
    "                    else:\n",
    "                        word_counts[word] = 1\n",
    "            \n",
    "            documents = [[word for word in doc if word_counts[word] >= self.min_frequency] for doc in documents]\n",
    "        \n",
    "        # Create a vocabulary from the documents\n",
    "        vocab = tf.keras.preprocessing.text.Tokenizer()\n",
    "        vocab.fit_on_texts(documents)\n",
    "        \n",
    "        # Store the vocabulary for later use\n",
    "        self.vocab_dict = {k: v-2 for k, v in vocab.word_index.items() if v is not None and (vocab.num_words is None or v <= vocab.num_words)}        \n",
    "        self.vocab_list = sorted(self.vocab_dict, key=self.vocab_dict.get)\n",
    "        self.reverse_vocab_dict = {v: k for k, v in self.vocab_dict.items()}\n",
    "\n",
    "    def transform(self, documents):\n",
    "        # Convert all text to lowercase\n",
    "        documents = [doc.lower() for doc in documents]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        documents = [nltk.word_tokenize(doc) for doc in documents]\n",
    "        \n",
    "        # Remove stop words\n",
    "        documents = [[word for word in doc if word not in self.stop_words] for doc in documents]\n",
    "        \n",
    "        # Lemmatize the text\n",
    "        documents = [[self.lemmatizer.lemmatize(word) for word in doc] for doc in documents]\n",
    "        \n",
    "        # Map words to their ids in the vocabulary\n",
    "        documents = [[self.vocab_dict.get(word, 0) for word in doc] for doc in documents]\n",
    "        \n",
    "        # Pad or truncate sentences to have a consistent length\n",
    "        if self.max_document_length is not None:\n",
    "            documents = tf.keras.preprocessing.sequence.pad_sequences(documents, maxlen=self.max_document_length, padding='post', truncating='post', value=0)\n",
    "        \n",
    "        return documents.tolist()\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(filepath): # a simple function to divide sentences and the similarity \n",
    "    print(\"Loading training data from \"+filepath)\n",
    "    x1=[]\n",
    "    x2=[]\n",
    "    y=[]\n",
    "    # positive samples from file\n",
    "    for line in open(filepath):\n",
    "        l=line.strip().split(\"\\t\")\n",
    "        if len(l)<2:\n",
    "            continue\n",
    "        if random() > 0.5:\n",
    "            x1.append(l[0].lower())\n",
    "            x2.append(l[1].lower())\n",
    "        else:\n",
    "            x1.append(l[1].lower())\n",
    "            x2.append(l[0].lower())\n",
    "        y.append(int(l[2]))\n",
    "    return np.asarray(x1),np.asarray(x2),np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpValidation(x1_text,x2_text,y,shuffled_index,val_idx,i): #leaving out some sentences to test later \n",
    "    print(\"dumping validation \"+str(i))\n",
    "    x1_shuffled=x1_text[shuffled_index]\n",
    "    x2_shuffled=x2_text[shuffled_index]\n",
    "    y_shuffled=y[shuffled_index]\n",
    "    x1_val=x1_shuffled[val_idx:]\n",
    "    x2_val=x2_shuffled[val_idx:]\n",
    "    y_val=y_shuffled[val_idx:]\n",
    "    del x1_shuffled\n",
    "    del y_shuffled\n",
    "    with open('validation.txt'+str(i),'w') as f:\n",
    "        for text1,text2,label in zip(x1_val,x2_val,y_val):\n",
    "            f.write(str(label)+\"\\t\"+text1+\"\\t\"+text2+\"\\n\")\n",
    "        f.close()\n",
    "    del x1_val\n",
    "    del y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSets(training_paths, max_document_length, percent_val, batch_size): #pour organiser mes donneesss\n",
    "\n",
    "    x1_text, x2_text, y= get_train(training_paths) # Splitting parts of each line in our data\n",
    "    # Build vocabulary\n",
    "    print(\"Building vocabulary\")\n",
    "    \n",
    "    \n",
    "    vocab_processor = VocabularyProcessor(max_document_length) \n",
    "    vocab_processor.fit_transform(np.concatenate((x2_text,x1_text),axis=0))\n",
    "    \n",
    "    \n",
    "    print(\"Length of loaded vocabulary ={}\".format(len(vocab_processor.vocab_list))) # Get the vocab size\n",
    "    \n",
    "    \n",
    "    train_set=[]\n",
    "    val_set=[]\n",
    "    \n",
    "    sum_no_of_batches = 0\n",
    "    \n",
    "    x1 = np.asarray(list(vocab_processor.transform(x1_text)))\n",
    "    x2 = np.asarray(list(vocab_processor.transform(x2_text)))\n",
    "    \n",
    "    \n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(131)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x1_shuffled = x1[shuffle_indices]\n",
    "    x2_shuffled = x2[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "    val_idx = -1*len(y_shuffled)*percent_val//100\n",
    "    del x1\n",
    "    del x2\n",
    "    \n",
    "    \n",
    "    # Saving for validation thingies\n",
    "    dumpValidation(x1_text,x2_text,y,shuffle_indices,val_idx,0)\n",
    "    \n",
    "    \n",
    "    # Getting random samples for each column\n",
    "    x1_train, x1_val = x1_shuffled[:val_idx], x1_shuffled[val_idx:]\n",
    "    x2_train, x2_val = x2_shuffled[:val_idx], x2_shuffled[val_idx:]\n",
    "    y_train, y_val = y_shuffled[:val_idx], y_shuffled[val_idx:]\n",
    "    print(\"Train/val split for {}: {:d}/{:d}\".format(training_paths, len(y_train), len(y_val)))\n",
    "    \n",
    "    sum_no_of_batches = sum_no_of_batches+(len(y_train)//batch_size)\n",
    "    \n",
    "    train_set=(x1_train,x2_train,y_train)\n",
    "    val_set=(x1_val,x2_val,y_val)\n",
    "    \n",
    "    gc.collect()\n",
    "    return train_set,val_set,vocab_processor,sum_no_of_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get max_doc_length \n",
    "def longest_sentence_length(sentences):\n",
    "\n",
    "    max_length = 0\n",
    "    for sentence in sentences:\n",
    "        length = len(sentence.split())\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "    return max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files= \"train_snli.txt\"     #for sentence semantic similarity \n",
    "max_document_length=1   #should be 78--longest sentence\n",
    "batch_size= 64     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from train_snli.txt\n"
     ]
    }
   ],
   "source": [
    "x1_text, x2_text, y= get_train(training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(longest_sentence_length(x2_text),longest_sentence_length(x2_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from train_snli.txt\n",
      "Building vocabulary\n",
      "Length of loaded vocabulary =26687\n",
      "dumping validation 0\n",
      "Train/val split for train_snli.txt: 330635/36738\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, vocab_processor,sum_no_of_batches = getDataSets(training_files,max_document_length, 10, batch_size)\n",
    "vocab_size = len(vocab_processor.vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "embedding_dim= 300 #Dimensionality of character embedding \n",
    "dropout_keep_prob= 1.0   # \"Dropout keep probability \n",
    "l2_reg_lambda= 0.0   # \"L2 regularizaion lambda \n",
    "hidden_units= 50    #\"Number of hidden units \n",
    "\n",
    "# Training parameters\n",
    "num_epochs= 300     #Number of training epochs (default: 200)\")\n",
    "learning_rate =1e-3\n",
    "\n",
    "train_x1, train_x2, train_y = train_set\n",
    "val_x1, val_x2, val_y = val_set\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Now i prepare optimizers with a clip to avoid exploading gradients \n",
    "from tensorflow.keras.optimizers import Adadelta, SGD, RMSprop, Adagrad, Adam, Adamax, Nadam\n",
    "\n",
    "gradient_clipping_norm = 1.25\n",
    "\n",
    "adadelta = Adadelta(clipnorm=gradient_clipping_norm, learning_rate=learning_rate)\n",
    "sgd = SGD(clipnorm=gradient_clipping_norm, learning_rate=learning_rate)\n",
    "rmsprop = RMSprop(clipnorm=gradient_clipping_norm, learning_rate = learning_rate)\n",
    "adagrad = Adagrad(clipnorm=gradient_clipping_norm, learning_rate=learning_rate)\n",
    "adam = Adam(clipnorm=gradient_clipping_norm, learning_rate = learning_rate)\n",
    "adamax = Adamax(clipnorm=gradient_clipping_norm, learning_rate=learning_rate)\n",
    "nadam = Nadam(clipnorm=gradient_clipping_norm, learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Subtract, Lambda, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "class SiameseLSTM1(tf.keras.Model):\n",
    "    \n",
    "    def __init__(\n",
    "        self, sequence_length, vocab_size, embedding_size, hidden_units, l2_reg_lambda, batch_size, train_set, dropout_rate, optimizer):\n",
    "        \n",
    "        super(SiameseLSTM1, self).__init__()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.batch_size = batch_size\n",
    "        self.train_set = train_set\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "        self.input_x1 = Input(shape=(None, self.embedding_size), name=\"input_x1\")\n",
    "        self.input_x2 = Input(shape=(None, self.embedding_size), name=\"input_x2\")\n",
    "        \n",
    "        # Define the embedding layer\n",
    "        self.embedding_layer = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_size, input_length=self.sequence_length)\n",
    "        \n",
    "        # Apply embedding layer to input sequences\n",
    "        self.embedded_x1 = self.embedding_layer(self.input_x1)\n",
    "        self.embedded_x2 = self.embedding_layer(self.input_x2)\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm_layer = LSTM(units=self.hidden_units, return_sequences=False)\n",
    "        \n",
    "        # Apply LSTM layer to embedded input sequences\n",
    "        self.encoded_x1 = self.lstm_layer(self.embedded_x1)\n",
    "        self.encoded_x2 = self.lstm_layer(self.embedded_x2)\n",
    "        \n",
    "        # Apply dropout layer\n",
    "        self.dropout_layer = Dropout(rate=self.dropout_rate)\n",
    "        self.dropout_x1 = self.dropout_layer(self.encoded_x1)\n",
    "        self.dropout_x2 = self.dropout_layer(self.encoded_x2)\n",
    "        \n",
    "        # Compute Manhattan distance between encoded input sequences\n",
    "        self.distance = Lambda(lambda x: tf.abs(x[0] - x[1]))([self.dropout_x1, self.dropout_x2])\n",
    "        \n",
    "        # Add BatchNormalization layer\n",
    "        self.bn_layer = BatchNormalization()\n",
    "        self.distance = self.bn_layer(self.distance)\n",
    "        \n",
    "        # Add more hidden layers\n",
    "        self.hidden_layer1 = Dense(units=64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg_lambda))(self.distance)\n",
    "        self.hidden_layer2 = Dense(units=32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(self.l2_reg_lambda))(self.hidden_layer1)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.output_layer = Dense(units=1, activation=\"sigmoid\", name=\"output\")(self.hidden_layer2)\n",
    "        \n",
    "        # Define the model and compile it\n",
    "        self.model = Model(inputs=[self.input_x1, self.input_x2], outputs=self.output_layer)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Compute class weights to handle class imbalance in training data\n",
    "        #class_weights = compute_sample_weight(class_weight='balanced', y=self.train_set[2])\n",
    "        self.model.compile(optimizer=self.optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, None, 300, 300)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# create model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m siamese_lstm \u001b[39m=\u001b[39mSiameseLSTM1(sequence_length\u001b[39m=\u001b[39;49m max_document_length, vocab_size \u001b[39m=\u001b[39;49m vocab_size, embedding_size \u001b[39m=\u001b[39;49m embedding_dim, hidden_units \u001b[39m=\u001b[39;49m hidden_units, l2_reg_lambda\u001b[39m=\u001b[39;49ml2_reg_lambda, batch_size\u001b[39m=\u001b[39;49mbatch_size, train_set\u001b[39m=\u001b[39;49mtrain_set,dropout_rate\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, optimizer\u001b[39m=\u001b[39;49msgd )\n",
      "Cell \u001b[1;32mIn[13], line 38\u001b[0m, in \u001b[0;36mSiameseLSTM1.__init__\u001b[1;34m(self, sequence_length, vocab_size, embedding_size, hidden_units, l2_reg_lambda, batch_size, train_set, dropout_rate, optimizer)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm_layer \u001b[39m=\u001b[39m LSTM(units\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_units, return_sequences\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Apply LSTM layer to embedded input sequences\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoded_x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedded_x1)\n\u001b[0;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoded_x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm_layer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedded_x2)\n\u001b[0;32m     41\u001b[0m \u001b[39m# Apply dropout layer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rania\\anaconda3\\envs\\mybase\\lib\\site-packages\\keras\\layers\\recurrent.py:659\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m _standardize_args(inputs,\n\u001b[0;32m    654\u001b[0m                                                      initial_state,\n\u001b[0;32m    655\u001b[0m                                                      constants,\n\u001b[0;32m    656\u001b[0m                                                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants)\n\u001b[0;32m    658\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 659\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(RNN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    661\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Rania\\anaconda3\\envs\\mybase\\lib\\site-packages\\keras\\engine\\base_layer.py:976\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[39m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[39m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[39m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[39m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[39mif\u001b[39;00m _in_functional_construction_mode(\u001b[39mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[1;32m--> 976\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m    977\u001b[0m                                             input_list)\n\u001b[0;32m    979\u001b[0m \u001b[39m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m    980\u001b[0m call_context \u001b[39m=\u001b[39m base_layer_utils\u001b[39m.\u001b[39mcall_context()\n",
      "File \u001b[1;32mc:\\Users\\Rania\\anaconda3\\envs\\mybase\\lib\\site-packages\\keras\\engine\\base_layer.py:1114\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     training_arg_passed_by_framework \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[39mwith\u001b[39;00m call_context\u001b[39m.\u001b[39menter(\n\u001b[0;32m   1112\u001b[0m     layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39minputs, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39mtraining_value):\n\u001b[0;32m   1113\u001b[0m   \u001b[39m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[1;32m-> 1114\u001b[0m   outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_tensor_symbolic_call(\n\u001b[0;32m   1115\u001b[0m       inputs, input_masks, args, kwargs)\n\u001b[0;32m   1117\u001b[0m   \u001b[39mif\u001b[39;00m outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mA layer\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39ms `call` method should return a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1119\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mTensor or a list of Tensors, not None \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1120\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m(layer: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m).\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rania\\anaconda3\\envs\\mybase\\lib\\site-packages\\keras\\engine\\base_layer.py:848\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(keras_tensor\u001b[39m.\u001b[39mKerasTensor, output_signature)\n\u001b[0;32m    847\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 848\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_output_signature(inputs, args, kwargs, input_masks)\n",
      "File \u001b[1;32mc:\\Users\\Rania\\anaconda3\\envs\\mybase\\lib\\site-packages\\keras\\engine\\base_layer.py:886\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_scope()):  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    881\u001b[0m   \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m    882\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m    883\u001b[0m     \u001b[39m# Build layer if applicable (if the `build` method has been\u001b[39;00m\n\u001b[0;32m    884\u001b[0m     \u001b[39m# overridden).\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[39m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[39;00m\n\u001b[1;32m--> 886\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_build(inputs)\n\u001b[0;32m    887\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[0;32m    888\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Rania\\anaconda3\\envs\\mybase\\lib\\site-packages\\keras\\engine\\base_layer.py:2633\u001b[0m, in \u001b[0;36mLayer._maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_build\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m   2631\u001b[0m   \u001b[39m# Check input assumptions set before layer building, e.g. input rank.\u001b[39;00m\n\u001b[0;32m   2632\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m-> 2633\u001b[0m     input_spec\u001b[39m.\u001b[39;49massert_input_compatibility(\n\u001b[0;32m   2634\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_spec, inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   2635\u001b[0m     input_list \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(inputs)\n\u001b[0;32m   2636\u001b[0m     \u001b[39mif\u001b[39;00m input_list \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype_policy\u001b[39m.\u001b[39mcompute_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Rania\\anaconda3\\envs\\mybase\\lib\\site-packages\\keras\\engine\\input_spec.py:214\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    212\u001b[0m   ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    213\u001b[0m   \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[1;32m--> 214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(input_index) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m of layer \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    215\u001b[0m                      layer_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m is incompatible with the layer: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    216\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(spec\u001b[39m.\u001b[39mndim) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, found ndim=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    217\u001b[0m                      \u001b[39mstr\u001b[39m(ndim) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m. Full shape received: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    218\u001b[0m                      \u001b[39mstr\u001b[39m(\u001b[39mtuple\u001b[39m(shape)))\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m   ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, None, 300, 300)"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "\n",
    "siamese_lstm =SiameseLSTM1(sequence_length= max_document_length, vocab_size = vocab_size, embedding_size = embedding_dim, hidden_units = hidden_units, l2_reg_lambda=l2_reg_lambda, batch_size=batch_size, train_set=train_set,dropout_rate=0.2, optimizer=sgd )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    history = siamese_lstm.model.fit(\n",
    "            x=[train_x1, train_x2],\n",
    "            y=train_y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=([val_x1, val_x2], val_y)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_lstm.model.save('models/SiamseseLSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(filepath, vocab_processor= vocab_processor): #Getting our test data\n",
    "    print(\"Loading testing/labelled data from \"+filepath +\" and preparing it for prediction \")\n",
    "    x1=[]\n",
    "    x2=[]\n",
    "    y=[]\n",
    "    # positive samples from file\n",
    "    for line in open(filepath):\n",
    "        l=line.strip().split(\"\\t\")\n",
    "        if len(l)<3: # Get only relevantly long sentences\n",
    "            continue\n",
    "        x1.append(l[1].lower())\n",
    "        x2.append(l[2].lower())\n",
    "        y.append(int(l[0])) \n",
    "        \n",
    "                \n",
    "    x1_test = np.asarray(list(vocab_processor.transform(x1)))\n",
    "    x2_test = np.asarray(list(vocab_processor.transform(x2)))\n",
    "    \n",
    "    \n",
    "    # Create an embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=len(vocab_processor.vocab_list) + 1,\n",
    "                                            output_dim=300)\n",
    "    \n",
    "\n",
    "    # Obtain word embeddings for the transformed new text data\n",
    "    new_word_embeddingsx1 = embedding_layer(x1_test)\n",
    "    new_word_embeddingsx2 = embedding_layer(x2_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    return new_word_embeddingsx1,new_word_embeddingsx2,np.asarray(y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, test_y = get_test(\"validation.txt0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def try_out_sentences(model):\n",
    "    # make a prediction using your trained Siamese LSTM model\n",
    "    prediction = model.predict([x1, x2])\n",
    "    \n",
    "    # we change dimension to evaluate our model when return sequences is true\n",
    "\n",
    "    #predictions_2d = predictions.mean(axis=1)\n",
    "    #predictions_1d = predictions_2d.flatten()\n",
    "    \n",
    "    #prediction = predictions_1d\n",
    "\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    \n",
    "    \n",
    "    acc = accuracy_score(test_y, prediction)\n",
    "    precision = precision_score(test_y, prediction)\n",
    "    recall = recall_score(test_y, prediction)\n",
    "    f1 = f1_score(test_y, prediction)\n",
    "\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1-score: \", f1)\n",
    "\n",
    "    \n",
    "    \n",
    "    return x1, x2, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x1, test_x2, test_y = try_out_sentences(siamese_lstm.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = siamese_lstm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict([test_x1, test_x2])\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = np.round(test_y)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict([test_x1, test_x2])\n",
    "\n",
    "# Compute fpr, tpr and auc\n",
    "fpr, tpr, thresholds = roc_curve(test_y, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict([test_x1, test_x2])\n",
    "\n",
    "# Compute precision, recall and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(test_y, y_pred)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='darkorange', lw=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
